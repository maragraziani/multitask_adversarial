{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "res_folders=os.listdir('../../results/')\n",
    "#model_folder='/home/mara/multitask_adversarial/results/NCOUNT_822/'\n",
    "CONCEPT=['domain']\n",
    "import keras\n",
    "keras.__version__\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(0)# str(hvd.local_rank())\n",
    "keras.backend.set_session(tf.Session(config=config))\n",
    "verbose=1 \n",
    "init=tf.global_variables_initializer() #initialize_all_variables()\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "#reducer = umap.UMAP()\n",
    "#reducer = reducer.fit(feats)\n",
    "#embedding = reducer.transform(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['domain']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONCEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder='/home/mara/multitask_adversarial/results/DOMAIN_822/'\n",
    "BASEL_FOLDER='../../results/BASEL_2009/best_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n",
      "Using brightness standardization\n"
     ]
    }
   ],
   "source": [
    "## Loading OS libraries to configure server preferences\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import setproctitle\n",
    "SERVER_NAME = 'ultrafast'\n",
    "EXPERIMENT_TYPE='test_domain'\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "## Adding PROCESS_UC1 utilities\n",
    "sys.path.append('../../lib/TASK_2_UC1/')\n",
    "from models import *\n",
    "from util import otsu_thresholding\n",
    "from extract_xml import *\n",
    "from functions import *                   \n",
    "sys.path.append('../../lib/')\n",
    "from mlta import *\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(0)# str(hvd.local_rank())\n",
    "keras.backend.set_session(tf.Session(config=config))\n",
    "\n",
    "verbose=1 \n",
    "\"\"\"loading dataset files\"\"\"\n",
    "#rank = MPI.COMM_WORLD.rank\n",
    "cam16 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/cam16_500/patches.h5py',  'r', libver='latest', swmr=True)\n",
    "all500 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/all500/patches.h5py',  'r', libver='latest', swmr=True)\n",
    "extra17 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/extra17/patches.h5py','r', libver='latest', swmr=True)\n",
    "tumor_extra17=hd.File('/home/mara/adversarialMICCAI/data/ultrafast/1129-1155/patches.h5py', 'r', libver='latest', swmr=True)\n",
    "test2 = hd.File('/mnt/nas2/results/IntermediateResults/Camelyon/ultrafast/test_data2/patches.hdf5', 'r', libver='latest', swmr=True)\n",
    "pannuke= hd.File('/mnt/nas2/results/IntermediateResults/Camelyon/pannuke/patches_fix.hdf5', 'r', libver='latest', swmr=True)\n",
    "\n",
    "global datasetss\n",
    "datasetss={'cam16':cam16,'all500':all500,'extra17':extra17, 'tumor_extra17':tumor_extra17, 'test_data2': test2, 'pannuke':pannuke}\n",
    "\n",
    "global concept_db\n",
    "concept_db = hd.File('../../data/normalized_cmeasures/concept_values_def.h5py','r')\n",
    "\n",
    "#SYSTEM CONFIGS \n",
    "CONFIG_FILE = '../../doc/config.cfg'\n",
    "COLOR = True\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# SAVE FOLD\n",
    "f=open(model_folder+\"/seed.txt\",\"r\")\n",
    "seed=1001#int(f.read())\n",
    "if verbose:  print(seed)\n",
    "#f.write(str(seed))\n",
    "f.close()\n",
    "\n",
    "\n",
    "# SET PROCESS TITLE\n",
    "setproctitle.setproctitle('{}'.format(EXPERIMENT_TYPE))\n",
    "\n",
    "# SET SEED\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# DATA SPLIT CSVs \n",
    "train_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/train_shuffle.csv', 'r') # How is the encoding of .csv files ?\n",
    "val_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/val_shuffle.csv', 'r')\n",
    "test_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/test_shuffle.csv', 'r')\n",
    "train_list=train_csv.readlines()\n",
    "val_list=val_csv.readlines()\n",
    "test_list=test_csv.readlines()\n",
    "test2_csv = open('/mnt/nas2/results/IntermediateResults/Camelyon/test2_shuffle.csv', 'r')\n",
    "test2_list=test2_csv.readlines()\n",
    "test2_csv.close()\n",
    "train_csv.close()\n",
    "val_csv.close()\n",
    "test_csv.close()\n",
    "data_csv=open('../../doc/pannuke_data_shuffle.csv', 'r')\n",
    "data_list=data_csv.readlines()\n",
    "data_csv.close()\n",
    "\n",
    "# STAIN NORMALIZATION\n",
    "def get_normalizer(patch, save_folder='../../results/'):\n",
    "    normalizer = ReinhardNormalizer()\n",
    "    normalizer.fit(patch)\n",
    "    np.save('{}/normalizer'.format(save_folder),normalizer)\n",
    "    np.save('{}/normalizing_patch'.format(save_folder), patch)\n",
    "    #print('Normalisers saved to disk.')\n",
    "    return normalizer\n",
    "def normalize_patch(patch, normalizer):\n",
    "    return np.float64(normalizer.transform(np.uint8(patch)))\n",
    "\n",
    "global normalizer\n",
    "db_name, entry_path, patch_no = get_keys(data_list[0])\n",
    "normalization_reference_patch = datasetss[db_name][entry_path][patch_no]\n",
    "normalizer = get_normalizer(normalization_reference_patch, save_folder='../../results/')\n",
    "\n",
    "\"\"\"\n",
    "Batch generators: \n",
    "They load a patch list: a list of file names and paths. \n",
    "They use the list to create a batch of 32 samples. \n",
    "\"\"\"\n",
    "\n",
    "# Retrieve Concept Measures\n",
    "def get_concept_measure(db_name, entry_path, patch_no, measure_type=''):\n",
    "    if measure_type=='domain':\n",
    "        return get_domain(db_name, entry_path)\n",
    "    path=db_name+'/'+entry_path+'/'+str(patch_no)+'/'+measure_type.strip(' ')\n",
    "    try:\n",
    "        cm=concept_db[path][0]\n",
    "        return cm\n",
    "    except:\n",
    "        print(\"[ERR]: {}, {}, {}, {} with path {}\".format(db_name, entry_path, patch_no, measure_type, path))\n",
    "        #import pdb; pdb.set_trace()\n",
    "        return 1.\n",
    "    \n",
    "# BATCH GENERATORS\n",
    "import keras.utils\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, patch_list, concept=CONCEPT, batch_size=32, shuffle=True, data_type=0):\n",
    "        self.batch_size=batch_size\n",
    "        self.patch_list=patch_list\n",
    "        self.shuffle=shuffle\n",
    "        self.concept = concept\n",
    "        self.data_type=data_type\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.patch_list)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes=self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        patch_list_temp=[self.patch_list[k] for k in indexes]\n",
    "        self.patch_list_temp=patch_list_temp\n",
    "        return self.__data_generation(self), None\n",
    "    \n",
    "    def get(self, index):\n",
    "        indexes=self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        patch_list_temp=[self.patch_list[k] for k in indexes]\n",
    "        self.patch_list_temp=patch_list_temp\n",
    "        return self.__data_generation(self), None\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.patch_list))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __data_generation(self, patch_list_temp):\n",
    "        patch_list_temp=self.patch_list_temp\n",
    "        batch_x=np.zeros((len(patch_list_temp), 224,224,3))\n",
    "        batch_y=np.zeros(len(patch_list_temp))\n",
    "        i=0\n",
    "        for line in patch_list_temp:\n",
    "            db_name, entry_path, patch_no = get_keys(line)\n",
    "            patch=datasetss[db_name][entry_path][patch_no]\n",
    "            patch=normalize_patch(patch, normalizer)\n",
    "            patch=keras.applications.inception_v3.preprocess_input(patch) \n",
    "            label = get_class(line, entry_path) \n",
    "            if self.data_type!=0:\n",
    "                label=get_test_label(entry_path)\n",
    "            batch_x[i]=patch\n",
    "            batch_y[i]=label\n",
    "            i+=1\n",
    "        generator_output=[batch_x, batch_y]\n",
    "        for c in self.concept:\n",
    "            batch_concept_values=np.zeros(len(patch_list_temp))\n",
    "            i=0\n",
    "            for line in patch_list_temp:\n",
    "                db_name, entry_path, patch_no = get_keys(line)\n",
    "                batch_concept_values[i]=get_concept_measure(db_name, entry_path, patch_no, measure_type=c)\n",
    "                i+=1\n",
    "            if c=='domain':\n",
    "                    batch_concept_values=keras.utils.to_categorical(batch_concept_values, num_classes=7)\n",
    "            generator_output.append(batch_concept_values)\n",
    "        return generator_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib as mpl   \n",
    "#mpl.use('Agg')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "from keras import *\n",
    "import setproctitle\n",
    "SERVER_NAME = 'ultrafast'\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "## Adding PROCESS_UC1 utilities\n",
    "sys.path.append('../../lib/TASK_2_UC1/')\n",
    "from models import *\n",
    "from util import otsu_thresholding\n",
    "from extract_xml import *\n",
    "from functions import *                   \n",
    "sys.path.append('../../lib/')\n",
    "from mlta import *\n",
    "import math\n",
    "import keras.callbacks as callbacks\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\"\"\"         \n",
    "Building guidable model \n",
    "\"\"\"\n",
    "def get_baseline_model(hp_lambda=0., domain=False, c_list=[]):\n",
    "    base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "    layers_list=['conv2d_92', 'conv2d_93', 'conv2d_88', 'conv2d_89', 'conv2d_86']\n",
    "    #layers_list=[]\n",
    "    for i in range(len(base_model.layers[:])):\n",
    "        layer=base_model.layers[i]\n",
    "        if layer.name in layers_list:\n",
    "            print layer.name\n",
    "            layer.trainable=True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "    feature_output=base_model.layers[-1].output\n",
    "    gap_layer_output = keras.layers.GlobalAveragePooling2D()(feature_output)\n",
    "    feature_output = Dense(2048, activation='relu', name='finetuned_features1',kernel_regularizer=keras.regularizers.l2(0.01))(gap_layer_output) \n",
    "    feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "    feature_output = Dense(512, activation='relu', name='finetuned_features2',kernel_regularizer=keras.regularizers.l2(0.01))(feature_output)\n",
    "    feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "    feature_output = Dense(256, activation='relu', name='finetuned_features3',kernel_regularizer=keras.regularizers.l2(0.01))(feature_output)\n",
    "    feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "    grl_layer=GradientReversal(hp_lambda=hp_lambda)\n",
    "    feature_output_grl = grl_layer(feature_output)\n",
    "    if domain:\n",
    "        domain_adversarial = keras.layers.Dense(7, activation = keras.layers.Activation('softmax'), name='domain_adversarial')(feature_output_grl)\n",
    "    finetuning = Dense(1,name='predictions')(feature_output)\n",
    "    ## here you need to check how many other concepts you have apart from domain adversarial\n",
    "    # then you add one layer per each. \n",
    "    if domain:\n",
    "        output_nodes=[finetuning, domain_adversarial]\n",
    "    else: \n",
    "        output_nodes=[finetuning]\n",
    "    for c in c_list:\n",
    "        if c!='domain':\n",
    "            concept_layer=  keras.layers.Dense(1, activation = keras.layers.Activation('linear'), name='extra_{}'.format(c.strip(' ')))(feature_output)\n",
    "            output_nodes.append(concept_layer)\n",
    "    model = Model(input=base_model.input, output=output_nodes)\n",
    "    model.grl_layer=grl_layer\n",
    "    return model\n",
    "\"\"\" \n",
    "Get trainable model with Hepistemic Uncertainty Weighted Loss \n",
    "\"\"\"\n",
    "# Custom loss layer\n",
    "from keras.initializers import Constant\n",
    "class CustomMultiLossLayer(Layer):\n",
    "    def __init__(self, nb_outputs=2, **kwargs):\n",
    "        self.nb_outputs = nb_outputs\n",
    "        self.is_placeholder = True\n",
    "        super(CustomMultiLossLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape=None):\n",
    "        self.log_vars = []\n",
    "        for i in range(self.nb_outputs):\n",
    "            self.log_vars += [self.add_weight(name='log_var' + str(i), shape=(1,), initializer=Constant(0.), trainable=True)]\n",
    "        super(CustomMultiLossLayer, self).build(input_shape)\n",
    "    \n",
    "    def multi_loss(self,  ys_true, ys_pred):\n",
    "        #print len(ys_true)\n",
    "        assert len(ys_true) == self.nb_outputs and len(ys_pred) == self.nb_outputs\n",
    "        loss = 0\n",
    "        i=0\n",
    "        for y_true, y_pred, log_var in zip(ys_true, ys_pred, self.log_vars):\n",
    "            precision =K.exp(-log_var[0]) ###MODIFICATION HERE\n",
    "            if i==0:\n",
    "                pred_loss = bbce(y_true, y_pred)\n",
    "                term = precision*pred_loss + 0.5 * log_var[0]  \n",
    "                #term=tf.Print(term, [term], 'bbce: ')\n",
    "            else:\n",
    "                pred_loss = keras_mse(y_true, y_pred)\n",
    "                #pred_loss=tf.Print(pred_loss, [pred_loss], 'MSE: ')\n",
    "                term = 0.5 * precision * pred_loss + 0.5 * log_var[0]\n",
    "                #term=tf.Print(term, [term], 'MSE: ')\n",
    "            loss+=term\n",
    "            term = 0.\n",
    "            i+=1\n",
    "        return K.mean(loss)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        ys_true = inputs[:self.nb_outputs]\n",
    "        ys_pred = inputs[self.nb_outputs:]\n",
    "        loss = self.multi_loss(ys_true, ys_pred)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return K.concatenate(inputs, -1)\n",
    "def get_trainable_model(baseline_model):\n",
    "    inp = keras.layers.Input(shape=(224,224,3,), name='inp')\n",
    "    y1_pred, y2_pred = baseline_model(inp)\n",
    "    y1_true=keras.layers.Input(shape=(1,),name='y1_true')\n",
    "    y2_true=keras.layers.Input(shape=(1,),name='y2_true')\n",
    "    out = CustomMultiLossLayer(nb_outputs=2)([y1_true, y2_true, y1_pred, y2_pred])\n",
    "    return Model(input=[inp, y1_true, y2_true], output=out)\n",
    "\n",
    "\"\"\" Get trainable model with Hepistemic Uncertainty Weighted Loss \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "LOSS FUNCTIONS\n",
    "\"\"\"\n",
    "def keras_mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.losses.mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def bbce(y_true, y_pred):\n",
    "    # we use zero weights to set the loss to zero for unlabeled data\n",
    "    verbose=0\n",
    "    zero= tf.constant(-1, dtype=tf.float32)\n",
    "    where = tf.not_equal(y_true, zero)\n",
    "    where = tf.reshape(where, [-1])\n",
    "    indices=tf.where(where) #indices where the item of y_true is NOT -1\n",
    "    indices = tf.reshape(indices, [-1])\n",
    "    sliced_y_true = tf.nn.embedding_lookup(y_true, indices)\n",
    "    sliced_y_pred = tf.nn.embedding_lookup(y_pred, indices)\n",
    "    n1 = tf.shape(indices)[0] #number of train images in batch\n",
    "    batch_size = tf.shape(y_true)[0]\n",
    "    n2 = batch_size - n1 #number of test images in batch\n",
    "    sliced_y_true = tf.reshape(sliced_y_true, [n1, -1])\n",
    "    n1_ = tf.cast(n1, tf.float32)\n",
    "    n2_ = tf.cast(n2, tf.float32)\n",
    "    multiplier = (n1_+ n2_) / n1_\n",
    "    zero_class = tf.constant(0, dtype=tf.float32)\n",
    "    where_class_is_zero=tf.cast(tf.reduce_sum(tf.cast(tf.equal(sliced_y_true, zero_class), dtype=tf.float32)), dtype=tf.float32)\n",
    "    if verbose:\n",
    "        where_class_is_zero=tf.Print(where_class_is_zero,[where_class_is_zero],'where_class_is_zero: ')\n",
    "    class_weight_zero = tf.cast(tf.divide(n1_, 2. * tf.cast(where_class_is_zero, dtype=tf.float32)+0.001), dtype=tf.float32)\n",
    "    \n",
    "    if verbose:\n",
    "        class_weight_zero=tf.Print(class_weight_zero,[class_weight_zero],'class_weight_zero: ')\n",
    "    one_class = tf.constant(1, dtype=tf.float32)\n",
    "    where_class_is_one=tf.cast(tf.reduce_sum(tf.cast(tf.equal(sliced_y_true, one_class), dtype=tf.float32)), dtype=tf.float32)\n",
    "    if verbose:\n",
    "        where_class_is_one=tf.Print(where_class_is_one,[where_class_is_one],'where_class_is_one: ')\n",
    "        n1_=tf.Print(n1_,[n1_],'n1_: ')\n",
    "    class_weight_one = tf.cast(tf.divide(n1_, 2. * tf.cast(where_class_is_one,dtype=tf.float32)+0.001), dtype=tf.float32)\n",
    "    class_weight_zero =  tf.constant(23477.0/(23477.0+123820.0), dtype=tf.float32)\n",
    "    class_weight_one =  tf.constant(123820.0/(23477.0+123820.0), dtype=tf.float32)\n",
    "    A = tf.ones(tf.shape(sliced_y_true), dtype=tf.float32) - sliced_y_true \n",
    "    A = tf.scalar_mul(class_weight_zero, A)\n",
    "    B = tf.scalar_mul(class_weight_one, sliced_y_true) \n",
    "    class_weight_vector=A+B\n",
    "    ce = tf.nn.sigmoid_cross_entropy_with_logits(labels=sliced_y_true,logits=sliced_y_pred)\n",
    "    ce = tf.multiply(class_weight_vector,ce)\n",
    "    return tf.reduce_mean(ce)\n",
    "\n",
    "from keras.initializers import Constant\n",
    "global domain_weight\n",
    "global main_task_weight\n",
    "\n",
    "class CustomMultiLossLayer(Layer):\n",
    "    def __init__(self, new_folder='', nb_outputs=2, **kwargs):\n",
    "        self.nb_outputs = nb_outputs\n",
    "        self.is_placeholder = True\n",
    "        super(CustomMultiLossLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape=None):\n",
    "        # initialise log_vars\n",
    "        self.log_vars = []\n",
    "        for i in range(self.nb_outputs):\n",
    "            self.log_vars += [self.add_weight(name='log_var' + str(i), shape=(1,),\n",
    "                                              initializer=Constant(0.), trainable=True)]\n",
    "        super(CustomMultiLossLayer, self).build(input_shape)\n",
    "    \"\"\"\n",
    "    def multi_loss(self, ys_true, ys_pred):\n",
    "        assert len(ys_true) == self.nb_outputs and len(ys_pred) == self.nb_outputs\n",
    "        loss = 0\n",
    "        for y_true, y_pred, log_var in zip(ys_true, ys_pred, self.log_vars):\n",
    "            precision = K.exp(-log_var[0])\n",
    "            loss += K.sum(precision * (y_true - y_pred)**2. + log_var[0], -1)\n",
    "        return K.mean(loss)\n",
    "    \"\"\"\n",
    "    def multi_loss(self,  ys_true, ys_pred):\n",
    "        assert len(ys_true) == self.nb_outputs and len(ys_pred) == self.nb_outputs\n",
    "        loss = 0\n",
    "        i=0\n",
    "        for y_true, y_pred, log_var in zip(ys_true, ys_pred, self.log_vars):\n",
    "            precision =keras.backend.exp(-log_var[0]) \n",
    "            if i==0:\n",
    "                pred_loss = bbce(y_true, y_pred)\n",
    "                term = main_task_weight*precision*pred_loss + main_task_weight*0.5 * log_var[0]  \n",
    "                #term=tf.Print(keras.backend.mean(term), [keras.backend.mean(term)], 'mean bbce: ')\n",
    "            #elif i==1:\n",
    "                # I need to find a better way for this\n",
    "            #    pred_loss = keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "                #keras_mse(y_true, y_pred)\n",
    "            #    term =  domain_weight * precision * pred_loss + domain_weight * log_var[0]\n",
    "                #term=tf.Print(keras.backend.mean(term), [keras.backend.mean(term)], 'mean cce: ')\n",
    "            else:\n",
    "                pred_loss = keras_mse(y_true, y_pred)\n",
    "                #pred_loss=tf.Print(pred_loss, [pred_loss], 'MSE: ')\n",
    "                term = 0.5 * precision * pred_loss + 0.5 * log_var[0]\n",
    "            loss+=term\n",
    "            term = 0.\n",
    "            i+=1\n",
    "        return keras.backend.mean(loss)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        ys_true = inputs[:self.nb_outputs]\n",
    "        ys_pred = inputs[self.nb_outputs:]\n",
    "        loss = self.multi_loss(ys_true, ys_pred)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return keras.backend.concatenate(inputs, -1)\n",
    "\n",
    "\"\"\"\n",
    "EVALUATION FUNCTIONs\n",
    "\"\"\"\n",
    "def accuracy_domain(y_true,y_pred):\n",
    "    y_p_r=np.round(y_pred)\n",
    "    acc = np.equal(y_p_r, y_true)**1.\n",
    "    acc = np.mean(np.float32(acc))\n",
    "    return acc\n",
    "def my_sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def my_accuracy_np(y_true, y_pred):\n",
    "    sliced_y_pred = my_sigmoid(y_pred)\n",
    "    y_pred_rounded = np.round(sliced_y_pred)\n",
    "    acc = np.equal(y_pred_rounded, y_true)**1.\n",
    "    acc = np.mean(np.float32(acc))\n",
    "    return acc\n",
    "def r_square_np(y_true, y_pred):\n",
    "    SS_res =  np.sum(np.square(y_true - y_pred))\n",
    "    SS_tot = np.sum(np.square(y_true - np.mean(y_true)))\n",
    "    r2_mine=( 1 - SS_res/(SS_tot + keras.backend.epsilon()) )\n",
    "    return ( 1 - SS_res/(SS_tot + keras.backend.epsilon()) )\n",
    "\n",
    "global report_val_acc \n",
    "global report_val_r2\n",
    "global report_val_mse\n",
    "report_val_acc=[]\n",
    "report_val_r2=[]\n",
    "report_val_mse=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_lambda=0.\n",
    "c_list=CONCEPT\n",
    "base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "layers_list=['conv2d_92', 'conv2d_93', 'conv2d_88', 'conv2d_89', 'conv2d_86']\n",
    "#layers_list=[]\n",
    "for i in range(len(base_model.layers[:])):\n",
    "    layer=base_model.layers[i]\n",
    "    if layer.name in layers_list:\n",
    "        print layer.name\n",
    "        layer.trainable=True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "feature_output=base_model.layers[-1].output\n",
    "gap_layer_output = keras.layers.GlobalAveragePooling2D()(feature_output)\n",
    "feature_output = keras.layers.Dense(2048, activation='relu', name='finetuned_features1',kernel_regularizer=keras.regularizers.l2(0.01))(gap_layer_output) \n",
    "feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "feature_output = keras.layers.Dense(512, activation='relu', name='finetuned_features2',kernel_regularizer=keras.regularizers.l2(0.01))(feature_output)\n",
    "feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "feature_output = keras.layers.Dense(256, activation='relu', name='finetuned_features3',kernel_regularizer=keras.regularizers.l2(0.01))(feature_output)\n",
    "#feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "grl_layer=GradientReversal(hp_lambda=hp_lambda)\n",
    "feature_output_grl = grl_layer(feature_output)\n",
    "domain_adversarial = keras.layers.Dense(7, activation = keras.layers.Activation('softmax'), name='domain_adversarial')(feature_output_grl)\n",
    "finetuning = Dense(1,name='predictions')(feature_output)\n",
    "#finetuning = keras.layers.Dense(1,name='predictions')(feature_output)\n",
    "#sfinetuning = Dense(1,name='predictions')(feature_output)\n",
    "## here you need to check how many other concepts you have apart from domain adversarial\n",
    "# then you add one layer per each. \n",
    "output_nodes=[finetuning, domain_adversarial]\n",
    "for c in c_list:\n",
    "    if c!='domain':\n",
    "        concept_layer=  keras.layers.Dense(1, activation = keras.layers.Activation('linear'), name='extra_{}'.format(c.strip(' ')))(feature_output)\n",
    "        output_nodes.append(concept_layer)\n",
    "model = Model(input=base_model.input, output=output_nodes)\n",
    "model.grl_layer=grl_layer\n",
    "#return model\n",
    "#regression_output = keras.layers.Dense(1, activation = keras.layers.Activation('linear'), name='concept_regressor')(gap_layer_output)\n",
    "#model = keras.Model(input=base_model.input, output=[finetuning, regression_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_task_weight=1. \n",
    "domain_weight = 1. #e-100\n",
    "t_m = get_trainable_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_m.load_weights('{}/best_model.h5'.format(model_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline=get_baseline_model()\n",
    "baseline.load_weights(BASEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151697"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['domain']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONCEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuned_features3\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "896\n",
      "928\n",
      "960\n",
      "992\n",
      "1024\n",
      "1056\n",
      "1088\n",
      "1120\n",
      "1152\n",
      "1184\n",
      "1216\n",
      "1248\n",
      "1280\n",
      "1312\n",
      "1344\n",
      "1376\n",
      "1408\n",
      "1440\n",
      "1472\n",
      "1504\n",
      "1536\n",
      "1568\n",
      "1600\n",
      "1632\n",
      "1664\n",
      "1696\n",
      "1728\n",
      "1760\n",
      "1792\n",
      "1824\n",
      "1856\n",
      "1888\n",
      "1920\n",
      "1952\n",
      "1984\n",
      "2016\n",
      "2048\n",
      "2080\n",
      "2112\n",
      "2144\n",
      "2176\n",
      "2208\n",
      "2240\n",
      "2272\n",
      "2304\n",
      "2336\n",
      "2368\n",
      "2400\n",
      "2432\n",
      "2464\n",
      "2496\n",
      "2528\n",
      "2560\n",
      "2592\n",
      "2624\n",
      "2656\n",
      "2688\n",
      "2720\n",
      "2752\n",
      "2784\n",
      "2816\n",
      "2848\n",
      "2880\n",
      "2912\n",
      "2944\n",
      "2976\n",
      "3008\n",
      "3040\n",
      "3072\n",
      "3104\n",
      "3136\n",
      "3168\n",
      "3200\n",
      "3232\n",
      "3264\n",
      "3296\n",
      "3328\n",
      "3360\n",
      "3392\n",
      "3424\n",
      "3456\n",
      "3488\n",
      "3520\n",
      "3552\n",
      "3584\n",
      "3616\n",
      "3648\n",
      "3680\n",
      "3712\n",
      "3744\n",
      "3776\n",
      "3808\n",
      "3840\n",
      "3872\n",
      "3904\n",
      "3936\n",
      "3968\n",
      "4000\n",
      "4032\n",
      "4064\n",
      "4096\n",
      "4128\n",
      "4160\n",
      "4192\n",
      "4224\n",
      "4256\n",
      "4288\n",
      "4320\n",
      "4352\n",
      "4384\n",
      "4416\n",
      "4448\n",
      "4480\n",
      "4512\n",
      "4544\n",
      "4576\n",
      "4608\n",
      "4640\n",
      "4672\n",
      "4704\n",
      "4736\n",
      "4768\n",
      "4800\n",
      "4832\n",
      "4864\n",
      "4896\n",
      "4928\n",
      "4960\n",
      "4992\n",
      "5024\n",
      "5056\n",
      "5088\n",
      "5120\n",
      "5152\n",
      "5184\n",
      "5216\n",
      "5248\n",
      "5280\n",
      "5312\n",
      "5344\n",
      "5376\n",
      "5408\n",
      "5440\n",
      "5472\n",
      "5504\n",
      "5536\n",
      "5568\n",
      "5600\n",
      "5632\n",
      "5664\n",
      "5696\n",
      "ERR\n",
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "896\n",
      "928\n",
      "960\n",
      "992\n",
      "1024\n",
      "1056\n",
      "1088\n",
      "1120\n",
      "1152\n",
      "1184\n",
      "1216\n",
      "1248\n",
      "1280\n",
      "1312\n",
      "1344\n",
      "1376\n",
      "1408\n",
      "1440\n",
      "1472\n",
      "1504\n",
      "1536\n",
      "1568\n",
      "1600\n",
      "1632\n",
      "1664\n",
      "1696\n",
      "1728\n",
      "1760\n",
      "1792\n",
      "1824\n",
      "1856\n",
      "1888\n",
      "1920\n",
      "1952\n",
      "1984\n",
      "2016\n",
      "2048\n",
      "2080\n",
      "2112\n",
      "2144\n",
      "2176\n",
      "2208\n",
      "2240\n",
      "2272\n",
      "2304\n",
      "2336\n",
      "2368\n",
      "2400\n",
      "2432\n",
      "2464\n",
      "2496\n",
      "2528\n",
      "2560\n",
      "2592\n",
      "2624\n",
      "2656\n",
      "2688\n",
      "2720\n",
      "2752\n",
      "2784\n",
      "2816\n",
      "2848\n",
      "2880\n",
      "2912\n",
      "2944\n",
      "2976\n",
      "3008\n",
      "3040\n",
      "3072\n",
      "3104\n",
      "3136\n",
      "3168\n",
      "3200\n",
      "3232\n",
      "3264\n",
      "3296\n",
      "3328\n",
      "3360\n",
      "3392\n",
      "3424\n",
      "3456\n",
      "3488\n",
      "3520\n",
      "3552\n",
      "3584\n",
      "3616\n",
      "3648\n",
      "3680\n",
      "3712\n",
      "3744\n",
      "3776\n",
      "3808\n",
      "3840\n",
      "3872\n",
      "3904\n",
      "3936\n",
      "3968\n",
      "4000\n",
      "4032\n",
      "4064\n",
      "4096\n",
      "4128\n",
      "4160\n",
      "4192\n",
      "4224\n",
      "4256\n",
      "4288\n",
      "4320\n",
      "4352\n",
      "4384\n",
      "4416\n",
      "4448\n",
      "4480\n",
      "4512\n",
      "4544\n",
      "4576\n",
      "4608\n",
      "4640\n",
      "4672\n",
      "4704\n",
      "4736\n",
      "4768\n",
      "4800\n",
      "4832\n",
      "4864\n",
      "4896\n",
      "4928\n",
      "4960\n",
      "4992\n",
      "5024\n",
      "5056\n",
      "5088\n",
      "5120\n",
      "5152\n",
      "5184\n",
      "5216\n",
      "5248\n",
      "5280\n",
      "5312\n",
      "5344\n",
      "5376\n",
      "5408\n",
      "5440\n",
      "5472\n",
      "5504\n",
      "5536\n",
      "5568\n",
      "5600\n",
      "5632\n",
      "5664\n",
      "5696\n",
      "ERR\n"
     ]
    }
   ],
   "source": [
    "type_='def_'\n",
    "test_list=test_list+test2_list\n",
    "for layer in ['finetuned_features3']:\n",
    "    print layer\n",
    "    get_guided_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.get_layer(layer).output])\n",
    "    get_base_layer_output = K.function([baseline.layers[0].input],\n",
    "                                  [baseline.get_layer(layer).output])\n",
    "    N_SAMPLES=len(train_list)\n",
    "    t_gen=DataGenerator(test_list, concept=CONCEPT, batch_size=BATCH_SIZE, data_type=1)\n",
    "    \n",
    "    shape=int(model.get_layer(layer).output.shape[1])\n",
    "    #import pdb; pdb.set_trace()\n",
    "    if layer=='mixed10':\n",
    "        shape=51200\n",
    "    feats = np.zeros((N_SAMPLES, shape))\n",
    "    i=0\n",
    "    labs=np.zeros(N_SAMPLES)\n",
    "    cvals=np.zeros((N_SAMPLES, 7))\n",
    "    #idxs = np.random.choice(np.arange(len(data_list)), N_SAMPLES, repetition=False)\n",
    "    batch_size=32\n",
    "    while i<=N_SAMPLES:\n",
    "        #import pdb; pdb.set_trace()\n",
    "        print i\n",
    "        input_,_=t_gen.__getitem__(i//batch_size)\n",
    "        try:\n",
    "            feats[i:i+batch_size, :] = get_guided_layer_output([input_[0]])[0].reshape(batch_size,-1)\n",
    "            labs[i:i+batch_size]=input_[1]\n",
    "            cvals[i:i+batch_size,:]=input_[2]\n",
    "            i+=batch_size\n",
    "            \n",
    "        except:\n",
    "            print \"ERR\"\n",
    "            #import pdb; pdb.set_trace()\n",
    "            break\n",
    "    np.save('{}/{}features{}.npy'.format(model_folder,type_, layer), feats)\n",
    "    np.save('{}/{}labels{}.npy'.format(model_folder, type_,layer), labs)\n",
    "    np.save('{}/{}cvalues{}.npy'.format(model_folder,type_, layer), cvals)\n",
    "    \n",
    "    t_gen=DataGenerator(test_list, concept=CONCEPT, batch_size=BATCH_SIZE, data_type=1)\n",
    "    #N_SAMPLES=5000\n",
    "    shape=int(model.get_layer(layer).output.shape[1])\n",
    "    if layer=='mixed10':\n",
    "        shape=51200\n",
    "    feats = np.zeros((N_SAMPLES, shape))\n",
    "    i=0\n",
    "    labs=np.zeros(N_SAMPLES)\n",
    "    cvals=np.zeros((N_SAMPLES,  7))\n",
    "    batch_size=32\n",
    "    while i+batch_size<=N_SAMPLES:\n",
    "        #import pdb; pdb.set_trace()\n",
    "        print i\n",
    "        input_,_=t_gen.__getitem__(i//batch_size)\n",
    "        try:\n",
    "            feats[i:i+batch_size, :] = get_base_layer_output([input_[0]])[0].reshape(batch_size,-1)\n",
    "            labs[i:i+batch_size]=input_[1]\n",
    "            cvals[i:i+batch_size,:]=input_[2]\n",
    "            i+=batch_size\n",
    "        except:\n",
    "            i+=batch_size\n",
    "            print \"ERR\"\n",
    "            break\n",
    "    np.save('{}/{}base_features{}.npy'.format(model_folder,type_,layer), feats)\n",
    "    np.save('{}/{}base_labels{}.npy'.format(model_folder,type_,layer), labs)\n",
    "    np.save('{}/{}base_cvalues{}.npy'.format(model_folder,type_,layer), cvals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mara/multitask_adversarial/results/DOMAIN//def_base_featuresfinetuned_features3.npy'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{}/{}base_features{}.npy'.format(model_folder,type_,layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
