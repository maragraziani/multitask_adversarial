{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STANDARD_NCOUNT\n",
      "NAREA_2409\n",
      "NCONTRAST_822\n",
      "F_CONTRAST_822\n",
      "NCONTRAST\n",
      "NCORRELATION_822\n",
      "NCOUNT_2009\n",
      "DOMAIN-COUNT_1139\n",
      "F_CORRELATION_1139\n",
      "NAREA_1139\n",
      "DOMAIN-COUNT_2009\n",
      "NCOUNT_2409\n",
      "NAREA_822\n",
      "F_CORRELATION_822\n",
      "F_CONTRAST_1139\n",
      "F_CONTRAST_2409\n",
      "NCONTRAST_1139\n",
      "NCORRELATION_2009\n",
      "NCONTRAST_2009\n",
      "NCORRELATION\n",
      "NCORRELATION_2409\n",
      "NAREA\n",
      "DOMAIN-COUNT_822\n",
      "NCONTRAST_2409\n",
      "NAREA_2009\n",
      "F_CORRELATION_2409\n",
      "DOMAIN-COUNT_2409\n",
      "DOMAIN-COUNT\n",
      "F_CORRELATION_2009\n",
      "F_CONTRAST_2009\n",
      "F_CORRELATION\n",
      "NCORRELATION_1139\n",
      "NCOUNT_822\n",
      "NCOUNT_1139\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "res_folders=os.listdir('../../results/')\n",
    "for res in res_folders:\n",
    "    if \"CONTRAST\" in res or \"COUNT\" in res or \"CORRELATION\" in res or \"AREA\" in res:\n",
    "        print res\n",
    "        #res = model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(0)# str(hvd.local_rank())\n",
    "keras.backend.set_session(tf.Session(config=config))\n",
    "verbose=1 \n",
    "init=tf.global_variables_initializer() #initialize_all_variables()\n",
    "sess=tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder='/home/mara/multitask_adversarial/results/NCOUNT_822/'\n",
    "CONCEPT=['ncount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../doc/data_shuffle.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'../../doc/data_shuffle.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822\n"
     ]
    }
   ],
   "source": [
    "## Loading OS libraries to configure server preferences\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import setproctitle\n",
    "SERVER_NAME = 'ultrafast'\n",
    "EXPERIMENT_TYPE='test_guided'\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "## Adding PROCESS_UC1 utilities\n",
    "sys.path.append('../../lib/TASK_2_UC1/')\n",
    "from models import *\n",
    "from util import otsu_thresholding\n",
    "from extract_xml import *\n",
    "from functions import *                   \n",
    "sys.path.append('../../lib/')\n",
    "from mlta import *\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(0)# str(hvd.local_rank())\n",
    "keras.backend.set_session(tf.Session(config=config))\n",
    "\n",
    "verbose=1 \n",
    "\"\"\"loading dataset files\"\"\"\n",
    "#rank = MPI.COMM_WORLD.rank\n",
    "cam16 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/cam16_500/patches.h5py',  'r', libver='latest', swmr=True)\n",
    "all500 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/all500/patches.h5py',  'r', libver='latest', swmr=True)\n",
    "extra17 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/extra17/patches.h5py','r', libver='latest', swmr=True)\n",
    "tumor_extra17=hd.File('/home/mara/adversarialMICCAI/data/ultrafast/1129-1155/patches.h5py', 'r', libver='latest', swmr=True)\n",
    "test2 = hd.File('/mnt/nas2/results/IntermediateResults/Camelyon/ultrafast/test_data2/patches.hdf5', 'r', libver='latest', swmr=True)\n",
    "pannuke= hd.File('/mnt/nas2/results/IntermediateResults/Camelyon/pannuke/patches_fix.hdf5', 'r', libver='latest', swmr=True)\n",
    "\n",
    "global datasetss\n",
    "datasetss={'cam16':cam16,'all500':all500,'extra17':extra17, 'tumor_extra17':tumor_extra17, 'test_data2': test2, 'pannuke':pannuke}\n",
    "\n",
    "global concept_db\n",
    "concept_db = hd.File('../../data/normalized_cmeasures/concept_values.h5py','r')\n",
    "#concept_db = hd.File('/mnt/nas2/results/IntermediateResults/Mara/MICCAI2020/MELBA_only_contrast_n.hd','r')\n",
    "#/mnt/nas2/results/IntermediateResults/Mara/MICCAI2020/MELBA_normalized_concepts.hd', 'r')\n",
    "# Note: nuclei_concepts not supported yet\n",
    "#global nuclei_concepts\n",
    "#nuclei_concepts=hd.File('/mnt/nas2/results/IntermediateResults/Mara/MICCAI2020/normalized_nuclei_concepts_db_new_try_def.hdf5','r')\n",
    "\n",
    "\n",
    "# Note: with uncertainty training we do not need CONCEPT ALPHAS (or LAMBDAs?) anymore\n",
    "# We still need to specify the learning rate of the gradient reversal (if we want to change it) or of the additional task.\n",
    "# I think we want to reduce it of a tenth or so\n",
    "\n",
    "#SYSTEM CONFIGS \n",
    "CONFIG_FILE = '../../doc/config.cfg'\n",
    "COLOR = True\n",
    "#global new_folder\n",
    "#new_folder = getFolderName()\n",
    "#folder_name=EXPERIMENT_TYPE\n",
    "#new_folder = 'results/'+folder_name #new_folder\n",
    "#os.mkdir(new_folder)\n",
    "\n",
    "# creating an ERR.log file to keep track of issues happened during model run\n",
    "#global error_log\n",
    "#error_log=open(new_folder+'/ERR.log', 'w')\n",
    "#llg.basicConfig(filename=os.path.join(ne\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# SAVE FOLD\n",
    "f=open(model_folder+\"/seed.txt\",\"r\")\n",
    "seed=int(f.read())\n",
    "if verbose:  print(seed)\n",
    "#f.write(str(seed))\n",
    "f.close()\n",
    "\n",
    "# SET PROCESS TITLE\n",
    "setproctitle.setproctitle('{}'.format(EXPERIMENT_TYPE))\n",
    "\n",
    "# SET SEED\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# DATA SPLIT CSVs \n",
    "train_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/train_shuffle.csv', 'r') # How is the encoding of .csv files ?\n",
    "val_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/val_shuffle.csv', 'r')\n",
    "test_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/test_shuffle.csv', 'r')\n",
    "train_list=train_csv.readlines()\n",
    "val_list=val_csv.readlines()\n",
    "test_list=test_csv.readlines()\n",
    "test2_csv = open('/mnt/nas2/results/IntermediateResults/Camelyon/test2_shuffle.csv', 'r')\n",
    "test2_list=test2_csv.readlines()\n",
    "test2_csv.close()\n",
    "train_csv.close()\n",
    "val_csv.close()\n",
    "test_csv.close()\n",
    "data_csv=open('../../doc/data_shuffle.csv', 'r')\n",
    "data_list=data_csv.readlines()\n",
    "data_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_sample(data, n_samples=2):\n",
    "    sample_=[data[i] for i in np.random.choice(len(data),n_samples)]\n",
    "    #sample_=[data[i] for i in range(len(data))]\n",
    "    return sample_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using brightness standardization\n"
     ]
    }
   ],
   "source": [
    "# STAIN NORMALIZATION\n",
    "def get_normalizer(patch, save_folder='../../results/'):\n",
    "    normalizer = ReinhardNormalizer()\n",
    "    normalizer.fit(patch)\n",
    "    np.save('{}/normalizer'.format(save_folder),normalizer)\n",
    "    np.save('{}/normalizing_patch'.format(save_folder), patch)\n",
    "    #print('Normalisers saved to disk.')\n",
    "    return normalizer\n",
    "def normalize_patch(patch, normalizer):\n",
    "    return np.float64(normalizer.transform(np.uint8(patch)))\n",
    "\n",
    "global normalizer\n",
    "db_name, entry_path, patch_no = get_keys(data_list[0])\n",
    "normalization_reference_patch = datasetss[db_name][entry_path][patch_no]\n",
    "normalizer = get_normalizer(normalization_reference_patch, save_folder='../../results/')\n",
    "\n",
    "\"\"\"\n",
    "Batch generators: \n",
    "They load a patch list: a list of file names and paths. \n",
    "They use the list to create a batch of 32 samples. \n",
    "\"\"\"\n",
    "# Retrieve Concept Measures\n",
    "def get_concept_measure(db_name, entry_path, patch_no, measure_type=''):\n",
    "    ### note: The measures in the file should have been scaled beforehand\n",
    "    # to have zero mean and unit std\n",
    "    \n",
    "    path=db_name+'/'+entry_path+'/'+str(patch_no)+'/'+measure_type\n",
    "    #import pdb; pdb.set_trace()\n",
    "    #print path\n",
    "    #print concept_db[path]\n",
    "    #print concept_db[path][0]\n",
    "    try:\n",
    "        cm=concept_db[path][0]\n",
    "        return cm\n",
    "    except:\n",
    "        print(\"[ERR]: {}, {}, {}, {} with path {}\".format(db_name, entry_path, patch_no, measure_type, path))\n",
    "        return 1.\n",
    "    \n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, patch_list, concept=CONCEPT, batch_size=32, shuffle=True, data_type=0):\n",
    "        self.batch_size=batch_size\n",
    "        self.patch_list=patch_list\n",
    "        self.shuffle=shuffle\n",
    "        self.concept = concept\n",
    "        self.data_type=data_type\n",
    "        print 'data type:', data_type\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.patch_list)/self.batch_size))\n",
    "    def __getitem__(self, index):\n",
    "        indexes=self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        patch_list_temp=[self.patch_list[k] for k in indexes]\n",
    "        #x, y, cm = self.__data_generation(self, patch_list_temp)\n",
    "        self.patch_list_temp=patch_list_temp\n",
    "        return self.__data_generation(self), None\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.patch_list))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def __data_generation(self, patch_list_temp):\n",
    "        patch_list_temp=self.patch_list_temp\n",
    "        batch_x=np.zeros((len(patch_list_temp), 224,224,3))\n",
    "        batch_y=np.zeros(len(patch_list_temp))\n",
    "        i=0\n",
    "        for line in patch_list_temp:\n",
    "            db_name, entry_path, patch_no = get_keys(line)\n",
    "            patch=datasetss[db_name][entry_path][patch_no]\n",
    "            patch=normalize_patch(patch, normalizer)\n",
    "            patch=keras.applications.inception_v3.preprocess_input(patch) \n",
    "            label = get_class(line, entry_path) \n",
    "            if self.data_type!=0:\n",
    "                label=get_test_label(entry_path)\n",
    "            batch_x[i]=patch\n",
    "            batch_y[i]=label\n",
    "            i+=1\n",
    "        #batch_x=np.asarray(batch_x, dtype=np.float32)\n",
    "        #batch_y=np.asarray(batch_y, dtype=np.float32)\n",
    "        #return [batch_x, batch_y, np.ones(len(patch_list_temp))], None\n",
    "        generator_output=[batch_x, batch_y]#np.ones(len(patch_list_temp)\n",
    "        for c in self.concept:\n",
    "            batch_concept_values=np.zeros(len(patch_list_temp))\n",
    "            i=0\n",
    "            for line in patch_list_temp:\n",
    "                db_name, entry_path, patch_no = get_keys(line)\n",
    "                #print 'line: {}, cmeasure: '.format(line),get_concept_measure(db_name, entry_path, patch_no, measure_type=c)\n",
    "                batch_concept_values[i]=get_concept_measure(db_name, entry_path, patch_no, measure_type=c)\n",
    "                i+=1\n",
    "            #print 'bcm: ', batch_concept_values\n",
    "            generator_output.append(batch_concept_values)\n",
    "        return generator_output\n",
    "\n",
    "def get_batch_data(patch_list, batch_size=32):\n",
    "    num_samples=len(patch_list)\n",
    "    while True:\n",
    "        offset = 0\n",
    "        for offset in range(0,num_samples, batch_size):\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            #concepts=['full_contrast','full_correlation','narea', 'ncount','nuclei_contrast','nuclei_correlation']\n",
    "            #for c in concepts:\n",
    "            #batch_concept_values[c]=[]\n",
    "            batch_samples=patch_list[offset:offset+batch_size]\n",
    "            for line in batch_samples[:(num_samples//batch_size)*batch_size]:\n",
    "                db_name, entry_path, patch_no = get_keys(line)\n",
    "                patch=datasetss[db_name][entry_path][patch_no]\n",
    "                patch=normalize_patch(patch, normalizer)\n",
    "                patch=keras.applications.inception_v3.preprocess_input(patch) \n",
    "                label = get_class(line, entry_path) \n",
    "                batch_x.append(patch)\n",
    "                batch_y.append(label)\n",
    "            batch_x=np.asarray(batch_x, dtype=np.float32)\n",
    "            batch_y=np.asarray(batch_y, dtype=np.float32)\n",
    "            generator_output=[batch_x, batch_y]\n",
    "            \n",
    "            for c in CONCEPT:\n",
    "                batch_concept_values=[]\n",
    "                for line in batch_samples[:(num_samples//batch_size)*batch_size]:\n",
    "                    batch_concept_values.append(get_concept_measure(db_name, entry_path, patch_no, measure_type=c))\n",
    "                batch_concept_values=np.asarray(batch_concept_values, dtype=np.float32)\n",
    "                generator_output.append(batch_concept_values)\n",
    "            #batch_domain=keras.utils.to_categorical(batch_domain, num_classes=6)\n",
    "            yield generator_output, None\n",
    "            \n",
    "def get_test_batch(patch_list, batch_size=32):\n",
    "    num_samples=len(patch_list)\n",
    "    while True:     \n",
    "        for offset in range(0,num_samples, batch_size):\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            #batch_concept_values={}\n",
    "            #concepts=['full_contrast','full_correlation','narea', 'ncount','nuclei_contrast','nuclei_correlation']\n",
    "            #for c in concepts:\n",
    "            #    batch_concept_values[c]=[]\n",
    "            batch_samples=patch_list[offset:offset+batch_size]\n",
    "            for line in batch_samples:\n",
    "                db_name, entry_path, patch_no = get_keys(line)\n",
    "                patch=datasetss[db_name][entry_path][patch_no]\n",
    "                patch=normalize_patch(patch, normalizer)\n",
    "                patch=keras.applications.inception_v3.preprocess_input(patch)\n",
    "                #patch=keras.applications.resnet50.preprocess_input(patch)\n",
    "                label = get_test_label(entry_path)\n",
    "                batch_x.append(patch)\n",
    "                batch_y.append(label)\n",
    "            batch_x=np.asarray(batch_x, dtype=np.float32)\n",
    "            batch_y=np.asarray(batch_y, dtype=np.float32)\n",
    "            generator_output=[batch_x, batch_y]\n",
    "            for c in CONCEPT:\n",
    "                batch_concept_values=[]\n",
    "                for line in batch_samples[:(num_samples//batch_size)*batch_size]:\n",
    "                    batch_concept_values.append(get_concept_measure(db_name, entry_path, patch_no, measure_type=c))\n",
    "                batch_concept_values=np.asarray(batch_concept_values, dtype=np.float32)\n",
    "                generator_output.append(batch_concept_values)\n",
    "            yield generator_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib as mpl   \n",
    "#mpl.use('Agg')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "from keras import *\n",
    "import setproctitle\n",
    "SERVER_NAME = 'ultrafast'\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "## Adding PROCESS_UC1 utilities\n",
    "sys.path.append('../../lib/TASK_2_UC1/')\n",
    "from models import *\n",
    "from util import otsu_thresholding\n",
    "from extract_xml import *\n",
    "from functions import *                   \n",
    "sys.path.append('../../lib/')\n",
    "from mlta import *\n",
    "import math\n",
    "import keras.callbacks as callbacks\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_88\n",
      "conv2d_89\n",
      "conv2d_92\n",
      "conv2d_93\n",
      "conv2d_86\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\"\"\"         \n",
    "Building guidable model \n",
    "\"\"\"\n",
    "\n",
    "base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "layers_list=['conv2d_92', 'conv2d_93', 'conv2d_88', 'conv2d_89', 'conv2d_86']\n",
    "#layers_list=[]\n",
    "for i in range(len(base_model.layers[:])):\n",
    "    layer=base_model.layers[i]\n",
    "    if layer.name in layers_list:\n",
    "        print layer.name\n",
    "        layer.trainable=True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "feature_output=base_model.layers[-1].output\n",
    "gap_layer_output = keras.layers.GlobalAveragePooling2D()(feature_output)\n",
    "feature_output = keras.layers.Dense(2048, activation='relu', name='finetuned_features1',kernel_regularizer=keras.regularizers.l2(0.01))(gap_layer_output) \n",
    "feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "feature_output = keras.layers.Dense(512, activation='relu', name='finetuned_features2',kernel_regularizer=keras.regularizers.l2(0.01))(feature_output)\n",
    "feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "feature_output = keras.layers.Dense(256, activation='relu', name='finetuned_features3',kernel_regularizer=keras.regularizers.l2(0.01))(feature_output)\n",
    "feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "finetuning = keras.layers.Dense(1,name='predictions')(feature_output)\n",
    "regression_output = keras.layers.Dense(1, activation = keras.layers.Activation('linear'), name='concept_regressor')(gap_layer_output)\n",
    "model = keras.Model(input=base_model.input, output=[finetuning, regression_output])\n",
    "# Callbacks\n",
    "def compute_mse(labels, predictions):\n",
    "    errors = labels - predictions\n",
    "    sum_squared_errors = np.sum(np.asarray([pow(errors[i],2) for i in range(len(errors))]))\n",
    "    mse = sum_squared_errors / len(labels)\n",
    "    return mse\n",
    "class eval_model(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        f = open('{}/val_by_epoch.txt'.format(new_folder), 'a')\n",
    "        pred_ = self.model.predict_generator(val_generator2, steps = len(val_list)// BATCH_SIZE, workers=4, use_multiprocessing=False)#// hvd.size()) \n",
    "        #import pdb; pdb.set_trace()\n",
    "        y_true = pred_[:,0]\n",
    "        y_pred = pred_[:,2]\n",
    "        print 'y_true: ', y_true\n",
    "        print 'y_pred: ', y_pred\n",
    "        val_acc = my_accuracy_np(y_true, y_pred)\n",
    "        cm_true = pred_[:, 1]\n",
    "        cm_pred = pred_[:,3]\n",
    "        print 'cm_true: ', cm_true\n",
    "        print 'cm_pred: ', cm_pred\n",
    "        \n",
    "        val_r2 = r_square_np(cm_true, cm_pred)\n",
    "        val_mse = compute_mse(cm_true, cm_pred)\n",
    "        print(\"Val acc: {}, r2: {}, MSE: {}\".format(val_acc, val_r2, val_mse))\n",
    "        report_val_acc.append(val_acc)\n",
    "        report_val_r2.append(val_r2)\n",
    "        report_val_mse.append(val_mse)\n",
    "        f.write(\"Val acc: {}, r2: {}, mse: {}\".format(val_acc, val_r2, val_mse))\n",
    "        \n",
    "        train_pred_ = self.model.predict_generator(train_generator2, steps=100, workers=1, use_multiprocessing=False)\n",
    "        cm_true = train_pred_[:, 1]\n",
    "        cm_pred = train_pred_[:,3]\n",
    "        print 'train_cmtrue: ', cm_true\n",
    "        print 'train_cmpred: ', cm_pred\n",
    "        train_r2 = r_square_np(cm_true, cm_pred)\n",
    "        train_mse = compute_mse(cm_true, cm_pred)\n",
    "        print(\"Train r2: {}, MSE: {}\".format(train_r2, train_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END Callbacks\n",
    "#\n",
    "def keras_mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.losses.mean_squared_error(y_true, y_pred))\n",
    "    #return tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def bbce(y_true, y_pred):\n",
    "    # we use zero weights to set the loss to zero for unlabeled data\n",
    "    verbose=0\n",
    "    zero= tf.constant(-1, dtype=tf.float32)\n",
    "    where = tf.not_equal(y_true, zero)\n",
    "    where = tf.reshape(where, [-1])\n",
    "    indices=tf.where(where) #indices where the item of y_true is NOT -1\n",
    "    indices = tf.reshape(indices, [-1])\n",
    "    sliced_y_true = tf.nn.embedding_lookup(y_true, indices)\n",
    "    sliced_y_pred = tf.nn.embedding_lookup(y_pred, indices)\n",
    "    n1 = tf.shape(indices)[0] #number of train images in batch\n",
    "    batch_size = tf.shape(y_true)[0]\n",
    "    n2 = batch_size - n1 #number of test images in batch\n",
    "    sliced_y_true = tf.reshape(sliced_y_true, [n1, -1])\n",
    "    n1_ = tf.cast(n1, tf.float32)\n",
    "    n2_ = tf.cast(n2, tf.float32)\n",
    "    multiplier = (n1_+ n2_) / n1_\n",
    "    zero_class = tf.constant(0, dtype=tf.float32)\n",
    "    where_class_is_zero=tf.cast(tf.reduce_sum(tf.cast(tf.equal(sliced_y_true, zero_class), dtype=tf.float32)), dtype=tf.float32)\n",
    "    if verbose:\n",
    "        where_class_is_zero=tf.Print(where_class_is_zero,[where_class_is_zero],'where_class_is_zero: ')\n",
    "    class_weight_zero = tf.cast(tf.divide(n1_, 2. * tf.cast(where_class_is_zero, dtype=tf.float32)+0.001), dtype=tf.float32)\n",
    "    \n",
    "    if verbose:\n",
    "        class_weight_zero=tf.Print(class_weight_zero,[class_weight_zero],'class_weight_zero: ')\n",
    "    one_class = tf.constant(1, dtype=tf.float32)\n",
    "    where_class_is_one=tf.cast(tf.reduce_sum(tf.cast(tf.equal(sliced_y_true, one_class), dtype=tf.float32)), dtype=tf.float32)\n",
    "    if verbose:\n",
    "        where_class_is_one=tf.Print(where_class_is_one,[where_class_is_one],'where_class_is_one: ')\n",
    "        n1_=tf.Print(n1_,[n1_],'n1_: ')\n",
    "    class_weight_one = tf.cast(tf.divide(n1_, 2. * tf.cast(where_class_is_one,dtype=tf.float32)+0.001), dtype=tf.float32)\n",
    "    class_weight_zero =  tf.constant(23477.0/(23477.0+123820.0), dtype=tf.float32)\n",
    "    class_weight_one =  tf.constant(123820.0/(23477.0+123820.0), dtype=tf.float32)\n",
    "    A = tf.ones(tf.shape(sliced_y_true), dtype=tf.float32) - sliced_y_true \n",
    "    A = tf.scalar_mul(class_weight_zero, A)\n",
    "    B = tf.scalar_mul(class_weight_one, sliced_y_true) \n",
    "    class_weight_vector=A+B\n",
    "    ce = tf.nn.sigmoid_cross_entropy_with_logits(labels=sliced_y_true,logits=sliced_y_pred)\n",
    "    ce = tf.multiply(class_weight_vector,ce)\n",
    "    return tf.reduce_mean(ce)\n",
    "\n",
    "# Custom loss layer\n",
    "from keras.initializers import Constant\n",
    "class CustomMultiLossLayer(Layer):\n",
    "    def __init__(self, nb_outputs=2, **kwargs):\n",
    "        self.nb_outputs = nb_outputs\n",
    "        self.is_placeholder = True\n",
    "        super(CustomMultiLossLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape=None):\n",
    "        self.log_vars = []\n",
    "        for i in range(self.nb_outputs):\n",
    "            self.log_vars += [self.add_weight(name='log_var' + str(i), shape=(1,), initializer=Constant(0.), trainable=True)]\n",
    "        super(CustomMultiLossLayer, self).build(input_shape)\n",
    "    \n",
    "    def multi_loss(self,  ys_true, ys_pred):\n",
    "        #print len(ys_true)\n",
    "        assert len(ys_true) == self.nb_outputs and len(ys_pred) == self.nb_outputs\n",
    "        loss = 0\n",
    "        i=0\n",
    "        for y_true, y_pred, log_var in zip(ys_true, ys_pred, self.log_vars):\n",
    "            precision =K.exp(-log_var[0]) ###MODIFICATION HERE\n",
    "            if i==0:\n",
    "                pred_loss = bbce(y_true, y_pred)\n",
    "                term = precision*pred_loss + 0.5 * log_var[0]  \n",
    "                #term=tf.Print(term, [term], 'bbce: ')\n",
    "            else:\n",
    "                pred_loss = keras_mse(y_true, y_pred)\n",
    "                #pred_loss=tf.Print(pred_loss, [pred_loss], 'MSE: ')\n",
    "                term = 0.5 * precision * pred_loss + 0.5 * log_var[0]\n",
    "                #term=tf.Print(term, [term], 'MSE: ')\n",
    "            loss+=term\n",
    "            term = 0.\n",
    "            i+=1\n",
    "        return K.mean(loss)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        ys_true = inputs[:self.nb_outputs]\n",
    "        ys_pred = inputs[self.nb_outputs:]\n",
    "        loss = self.multi_loss(ys_true, ys_pred)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return K.concatenate(inputs, -1)\n",
    "def get_trainable_model(baseline_model):\n",
    "    inp = keras.layers.Input(shape=(224,224,3,), name='inp')\n",
    "    y1_pred, y2_pred = baseline_model(inp)\n",
    "    y1_true=keras.layers.Input(shape=(1,),name='y1_true')\n",
    "    y2_true=keras.layers.Input(shape=(1,),name='y2_true')\n",
    "    out = CustomMultiLossLayer(nb_outputs=2)([y1_true, y2_true, y1_pred, y2_pred])\n",
    "    return Model(input=[inp, y1_true, y2_true], output=out)\n",
    "\n",
    "\"\"\" Get trainable model with Hepistemic Uncertainty Weighted Loss \"\"\"\n",
    "t_m = get_trainable_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_m.load_weights(model_folder+'/best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inp (InputLayer)                (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "y1_true (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "y2_true (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 [(None, 1), (None, 1 27181858    inp[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "custom_multi_loss_layer_1 (Cust [(None, 1), (None, 1 2           y1_true[0][0]                    \n",
      "                                                                 y2_true[0][0]                    \n",
      "                                                                 model_1[1][0]                    \n",
      "                                                                 model_1[1][1]                    \n",
      "==================================================================================================\n",
      "Total params: 27,181,860\n",
      "Trainable params: 7,803,908\n",
      "Non-trainable params: 19,377,952\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "t_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def evaluate_model(d_list, model, batch_size=BATCH_SIZE, test_type=''):\n",
    "    batch_size=32\n",
    "    #d_list=test_list[:100]\n",
    "    #t_gen=DataGenerator(d_list, concept=CONCEPT, batch_size=BATCH_SIZE, data_type=0)\n",
    "    test_generator_=get_test_batch(d_list, batch_size=batch_size)\n",
    "    steps=len(d_list)//batch_size\n",
    "    print steps\n",
    "    initial_lr = 1e-4\n",
    "    opt = keras.optimizers.SGD(lr=initial_lr, momentum=0.9, nesterov=True)\n",
    "    compile_model(t_m,opt,loss=None,metrics=None)\n",
    "    callbacks = []\n",
    "    y_true=np.zeros(len(d_list))\n",
    "    y_pred=np.zeros((len(d_list),1))\n",
    "    N=0\n",
    "    all_cm=np.zeros(len(d_list))\n",
    "    all_p_cm=np.zeros(len(d_list))\n",
    "\n",
    "    while N<len(d_list):\n",
    "        #x_b, y_b, cm_b = t_gen.__getitem__(N)\n",
    "        [x_b, y_b, cm_b], _ = test_generator_.next()\n",
    "        pred_ = t_m.predict([x_b, y_b, cm_b])\n",
    "        y_p_b = pred_[:,2]\n",
    "        cm_p_b = pred_[:,3]\n",
    "\n",
    "        y_true[N:N+len(y_b)]=y_b.reshape(len(y_b))\n",
    "        y_pred[N:N+len(y_p_b)]=y_p_b.reshape(len(y_p_b),1)\n",
    "        all_p_cm[N:N+len(cm_p_b)]=cm_p_b.reshape(len(cm_p_b))\n",
    "        all_cm[N:N+len(cm_b)]=cm_b\n",
    "        N+=len(y_p_b)\n",
    "    y_true=y_true.reshape((len(d_list),1))\n",
    "    acc = my_accuracy(y_true, y_pred).eval(session=tf.Session())\n",
    "    sliced_y_pred = tf.sigmoid(y_pred)\n",
    "    y_pred_rounded = K.round(sliced_y_pred)\n",
    "    acc_sc = accuracy_score(y_pred_rounded.eval(session=tf.Session()), y_true)\n",
    "    print('accuracy: ', acc_sc)\n",
    "    \n",
    "    y_pred = sliced_y_pred.eval(session=tf.Session())\n",
    "    #sliced_y_pred = tf.sigmoid(y_pred)\n",
    "    #y_pred_rounded = K.round(sliced_y_pred)\n",
    "    auc_score=sklearn.metrics.roc_auc_score(y_true,sliced_y_pred.eval(session=tf.Session()))\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(1):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc_score\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example AUC = {}'.format(roc_auc[0]))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show() \n",
    "    print 'auc: {}'.format(auc_score)\n",
    "    print roc_auc[0]\n",
    "    #auc_record = open('{}/auc_{}.txt'.format(model_folder,test_type), 'w')\n",
    "    #auc_record.write('{}'.format(roc_auc[0]))\n",
    "    #auc_record.close()\n",
    "    return all_cm, all_p_cm, auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Main Task\n",
    "##### How do we do on the patch classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "('accuracy: ', 0.858)\n",
      "auc: 0.903796798306\n"
     ]
    }
   ],
   "source": [
    "keras.backend.get_session().run(tf.global_variables_initializer())\n",
    "t_m.load_weights(model_folder+'/best_model.h5')\n",
    "all_cm_t, all_p_cm_t, roc_auc=evaluate_model(train_list[:1000],t_m, test_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "('accuracy: ', 0.637)\n",
      "auc: 0.79626\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-be61b67186f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_cm_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_p_cm_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest2_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'external'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-2e4a5a7f1741>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(d_list, model, batch_size, test_type)\u001b[0m\n\u001b[1;32m     66\u001b[0m     plt.show()'''    \n\u001b[1;32m     67\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'auc: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;31m#auc_record = open('{}/auc_{}.txt'.format(model_folder,test_type), 'w')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m#auc_record.write('{}'.format(roc_auc[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "all_cm_t, all_p_cm_t, roc_auc=evaluate_model(test2_list,t_m, test_type='external')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "('accuracy: ', 0.7697126013264555)\n",
      "auc: 0.805124952986\n"
     ]
    }
   ],
   "source": [
    "keras.backend.get_session().run(tf.global_variables_initializer())\n",
    "t_m.load_weights(model_folder+'/best_model.h5')\n",
    "all_cm_t, all_p_cm_t, roc_auc=evaluate_model(test_list,t_m, test_type='internal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "('accuracy: ', 0.7722918201915991)\n",
      "auc: 0.810951137265\n",
      "84\n",
      "('accuracy: ', 0.7811348563006633)\n",
      "auc: 0.815366490778\n",
      "84\n",
      "('accuracy: ', 0.762343404568902)\n",
      "auc: 0.791973538127\n",
      "84\n",
      "('accuracy: ', 0.7593957258658807)\n",
      "auc: 0.79625658066\n",
      "84\n",
      "('accuracy: ', 0.7678703021370671)\n",
      "auc: 0.8005769738\n",
      "84\n",
      "('accuracy: ', 0.7811348563006633)\n",
      "auc: 0.805986774018\n",
      "84\n",
      "('accuracy: ', 0.7733971997052321)\n",
      "auc: 0.80673391482\n",
      "84\n",
      "('accuracy: ', 0.7689756816507001)\n",
      "auc: 0.80471903359\n",
      "84\n",
      "('accuracy: ', 0.7767133382461312)\n",
      "auc: 0.808131745227\n",
      "84\n",
      "('accuracy: ', 0.7719233603537214)\n",
      "auc: 0.799254467893\n",
      "84\n",
      "('accuracy: ', 0.7756079587324981)\n",
      "auc: 0.809515090331\n",
      "84\n",
      "('accuracy: ', 0.7697126013264555)\n",
      "auc: 0.805486182302\n",
      "84\n",
      "('accuracy: ', 0.7741341193809874)\n",
      "auc: 0.79661071668\n",
      "84\n",
      "('accuracy: ', 0.7789240972733972)\n",
      "auc: 0.823377377876\n",
      "84\n",
      "('accuracy: ', 0.7704495210022108)\n",
      "auc: 0.808077429984\n",
      "84\n",
      "('accuracy: ', 0.7652910832719234)\n",
      "auc: 0.806820947366\n",
      "84\n",
      "('accuracy: ', 0.7774502579218865)\n",
      "auc: 0.808956523079\n",
      "84\n",
      "('accuracy: ', 0.7848194546794399)\n",
      "auc: 0.811746902933\n",
      "84\n",
      "('accuracy: ', 0.7476050110537952)\n",
      "auc: 0.777359701702\n",
      "84\n",
      "('accuracy: ', 0.7564480471628593)\n",
      "auc: 0.784371645174\n",
      "84\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, nuclei_correlation with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/nuclei_correlation\n",
      "('accuracy: ', 0.7730287398673544)\n",
      "auc: 0.81263910706\n",
      "84\n",
      "('accuracy: ', 0.7719233603537214)\n",
      "auc: 0.804632279971\n",
      "84\n",
      "('accuracy: ', 0.7763448784082535)\n",
      "auc: 0.810920529976\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "aucs_i=[]\n",
    "for i in range(100):\n",
    "    test_list_b=get_bootstrap_sample(test_list, n_samples=len(test_list))\n",
    "    all_cm_i, all_p_cm_i, roc_auc=evaluate_model(test_list_b,t_m, test_type='internal')\n",
    "    aucs_i.append(roc_auc)\n",
    "print \"AUC avg (std): {} ({})\".format(np.mean(aucs_i), np.std(aucs_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_e=[]\n",
    "for i in range(100):\n",
    "    test2_list_b=get_bootstrap_sample(test2_list, n_samples=len(test2_list))\n",
    "    all_cm_e, all_p_cm_e, roc_auc=evaluate_model(test2_list_b,t_m, test_type='external')\n",
    "    aucs_e.append(roc_auc)\n",
    "print \"AUC avg (std): {} ({})\".format(np.mean(aucs_e), np.std(aucs_e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on Auxiliary tasks\n",
    "## Are we learning the concepts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rsquared(labels, predictions):\n",
    "    errors = labels - predictions\n",
    "    sum_squared_errors = np.sum(np.asarray([pow(errors[i],2) for i in range(len(errors))]))\n",
    "    # total sum of squares, TTS\n",
    "    average_y = np.mean(labels)\n",
    "    total_errors = labels - average_y\n",
    "    total_sum_squares = np.sum(np.asarray([pow(total_errors[i],2) for i in range(len(total_errors))]))\n",
    "    #rsquared is 1-RSS/TTS\n",
    "    rss_over_tts =   sum_squared_errors/total_sum_squares\n",
    "    rsquared = 1-rss_over_tts\n",
    "    return rsquared\n",
    "def compute_mse(labels, predictions):\n",
    "    errors = labels - predictions\n",
    "    sum_squared_errors = np.sum(np.asarray([pow(errors[i],2) for i in range(len(errors))]))\n",
    "    mse = sum_squared_errors / len(labels)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_i = compute_rsquared(all_cm_i, all_p_cm_i)\n",
    "mse_i = compute_mse(all_cm_i, all_p_cm_i)\n",
    "print 'Internal: ', r2_i, mse_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_e = compute_rsquared(all_cm_e, all_p_cm_e)\n",
    "mse_e = compute_mse(all_cm_e, all_p_cm_e)\n",
    "print 'External: ', r2_e, mse_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_type='internal'\n",
    "auc_record = open('{}/concept_metrics_{}.txt'.format(model_folder,test_type), 'w')\n",
    "auc_record.write('{}, {}'.format(r2_i, mse_i))\n",
    "auc_record.close()\n",
    "test_type='external'\n",
    "auc_record = open('{}/concept_metrics_{}.txt'.format(model_folder,test_type), 'w')\n",
    "auc_record.write('{}, {}'.format(r2_e, mse_e))\n",
    "auc_record.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_t = compute_rsquared(all_cm_t, all_p_cm_t)\n",
    "mse_t = compute_mse(all_cm_t, all_p_cm_t)\n",
    "print 'Train: ', r2_t, mse_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_r2=np.load('{}/val_r2_log.npy'.format(model_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=np.load('{}/training_log.npy'.format(model_folder), allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('{}/val_by_epoch.txt'.format(model_folder), 'r')\n",
    "f_l=f.readlines()\n",
    "val_acc=[]\n",
    "val_r2=[]\n",
    "val_mse=[]\n",
    "for line in f_l:\n",
    "    acc=line.split('Val acc: ')[1].split(', r2')[0]\n",
    "    val_acc.append(acc)\n",
    "    r2=line.split(', r2:')[1].split(', mse:')[0]\n",
    "    mse=line.split(', mse:')[1].split('\\n')[0]\n",
    "    val_r2.append(r2)\n",
    "    val_mse.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.asarray(val_acc, dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.asarray(val_r2, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.asarray(val_mse, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
