{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "## Loading OS libraries to configure server preferences\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import setproctitle\n",
    "SERVER_NAME = 'ultrafast'\n",
    "EXPERIMENT_TYPE='test_baseline'\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "## Adding PROCESS_UC1 utilities\n",
    "sys.path.append('../../lib/TASK_2_UC1/')\n",
    "from models import *\n",
    "from util import otsu_thresholding\n",
    "from extract_xml import *\n",
    "from functions import *                   \n",
    "sys.path.append('../../lib/')\n",
    "from mlta import *\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = '0'\n",
    "keras.backend.set_session(tf.Session(config=config))\n",
    "\n",
    "verbose=1 \n",
    "\n",
    "cam16 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/cam16_500/patches.hdf5',  'r', libver='latest', swmr=True)\n",
    "all500 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/all500/patches.hdf5',  'r', libver='latest', swmr=True)\n",
    "extra17 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/extra17/patches.hdf5',  'r', libver='latest', swmr=True)\n",
    "tumor_extra17=hd.File('/home/mara/adversarialMICCAI/data/ultrafast/1129-1155/patches.hdf5', 'r', libver='latest', swmr=True)\n",
    "test2 = hd.File('/mnt/nas2/results/IntermediateResults/Camelyon/ultrafast/test_data2/patches.hdf5', 'r', libver='latest', swmr=True)\n",
    "pannuke= hd.File('/mnt/nas2/results/IntermediateResults/Camelyon/pannuke/patches_fix.hdf5', 'r', libver='latest', swmr=True)\n",
    "\n",
    "global data\n",
    "data={'cam16':cam16,'all500':all500,'extra17':extra17, 'tumor_extra17':tumor_extra17, 'test_data2': test2, 'pannuke':pannuke}\n",
    "global concept_db\n",
    "concept_db = hd.File('/mnt/nas2/results/IntermediateResults/Mara/MICCAI2020/MELBA_normalized_concepts.hd', 'r')\n",
    "# Note: nuclei_concepts not supported yet\n",
    "global nuclei_concepts\n",
    "nuclei_concepts=hd.File('/mnt/nas2/results/IntermediateResults/Mara/MICCAI2020/MELBA_normalized_concepts.hd','r')\n",
    "\n",
    "#SYSTEM CONFIGS \n",
    "CONFIG_FILE = 'doc/config.cfg'\n",
    "COLOR = True\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "seed=1\n",
    "print seed\n",
    "\n",
    "# SET PROCESS TITLE\n",
    "setproctitle.setproctitle('{}'.format(EXPERIMENT_TYPE))\n",
    "\n",
    "# SET SEED\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# DATA SPLIT CSVs \n",
    "train_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/train_shuffle.csv', 'r') # How is the encoding of .csv files ?\n",
    "val_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/val_shuffle.csv', 'r')\n",
    "test_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/test_shuffle.csv', 'r')\n",
    "train_list=train_csv.readlines()\n",
    "val_list=val_csv.readlines()\n",
    "test_list=test_csv.readlines()\n",
    "test2_csv = open('/mnt/nas2/results/IntermediateResults/Camelyon/test2_shuffle.csv', 'r')\n",
    "test2_list=test2_csv.readlines()\n",
    "test2_csv.close()\n",
    "train_csv.close()\n",
    "val_csv.close()\n",
    "test_csv.close()\n",
    "#data_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/data_shuffle.csv', 'r')\n",
    "#data_csv=open('./data/train.csv', 'r')\n",
    "data_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/pannuke/pannuke_train_shuffled.csv', 'r')\n",
    "data_list=data_csv.readlines()\n",
    "data_csv.close()\n",
    "\n",
    "# STAIN NORMALIZATION\n",
    "def get_normalizer(patch, save_folder=''):\n",
    "    normalizer = ReinhardNormalizer()\n",
    "    normalizer.fit(patch)\n",
    "    np.save('{}/normalizer'.format(save_folder),normalizer)\n",
    "    np.save('{}/normalizing_patch'.format(save_folder), patch)\n",
    "    print('Normalisers saved to disk.')\n",
    "    return normalizer\n",
    "\n",
    "def normalize_patch(patch, normalizer):\n",
    "    return np.float64(normalizer.transform(np.uint8(patch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using brightness standardization\n",
      "Normalisers saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA NORMALIZER\n",
    "global normalizer\n",
    "db_name, entry_path, patch_no = get_keys(data_list[0])\n",
    "normalization_reference_patch = data[db_name][entry_path][patch_no]\n",
    "normalizer = get_normalizer(normalization_reference_patch, save_folder='./')\n",
    "# Retrieve Concept Measures\n",
    "def get_concept_measure(db_name, entry_path, patch_no, measure_type=''):\n",
    "    ### note: The measures in the file should have been scaled beforehand\n",
    "    # to have zero mean and unit std\n",
    "    if db_name=='pannuke':\n",
    "        #import pdb; pdb.set_trace()\n",
    "        try:\n",
    "            cm=concept_db[entry_path+'  /'+measure_type][0]\n",
    "            #print 'pannuke ', cm\n",
    "            return cm\n",
    "        except:\n",
    "            print \"[ERR]: {}, {}, {}, {}\".format(db_name, entry_path, patch_no, measure_type)\n",
    "            print entry_path+'  /'+measure_type\n",
    "            return 1.\n",
    "    else:\n",
    "        try: \n",
    "            cm=concept_db[db_name+'/'+entry_path+'/'+str(patch_no)+'/'+measure_type][0]\n",
    "            #print 'other ', cm\n",
    "            return cm\n",
    "        except:\n",
    "            print \"[ERR]: {}, {}, {}, {}\".format(db_name, entry_path, patch_no, measure_type)\n",
    "            #error_log.write('[get_concept_measure] {}, {}, {}, {}'.format(db_name, entry_path, patch_no, measure_type))\n",
    "            return 1.\n",
    "def get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type=''):\n",
    "    ### note: The measures in the file should have been scaled beforehand\n",
    "    # to have zero mean and unit std\n",
    "    try:\n",
    "        cm = nuclei_concepts[db_name+'/'+entry_path+'/'+str(patch_no)+'/'+measure_type][0]\n",
    "    except:\n",
    "        #error_log.write('[get_segmented_concept_measure] {}, {}, {}, {}'.format(db_name, entry_path, patch_no, measure_type))\n",
    "        print \"[ERROR] Issue retreiving concept measure for {}, {}, {}, {}\".format(db_name, entry_path, patch_no, measure_type)\n",
    "        return 1.\n",
    "\n",
    "# BATCH GENERATORS\n",
    "def get_batch_data(patch_list, batch_size=32):\n",
    "    num_samples=len(patch_list)\n",
    "    while True:\n",
    "        offset = 0\n",
    "        for offset in range(0,num_samples, batch_size):\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            batch_contrast=[]\n",
    "            batch_samples=patch_list[offset:offset+batch_size]\n",
    "            for line in batch_samples[:(num_samples//batch_size)*batch_size]:\n",
    "                db_name, entry_path, patch_no = get_keys(line)\n",
    "                patch=data[db_name][entry_path][patch_no]\n",
    "                patch=normalize_patch(patch, normalizer)\n",
    "                patch=keras.applications.inception_v3.preprocess_input(patch) \n",
    "                label = get_class(line, entry_path) \n",
    "                batch_x.append(patch)\n",
    "                batch_y.append(label)\n",
    "                # ONES\n",
    "                #batch_ones.append(1.)\n",
    "                # NOISE\n",
    "                #batch_noise.append(np.random.normal(0.))\n",
    "                # CONCEPT = contrast\n",
    "                batch_contrast.append(get_concept_measure(db_name, entry_path, patch_no, measure_type='norm_contrast'))\n",
    "                # CONCEPT = domain\n",
    "                #batch_domain.append(get_domain(db_name, entry_path))\n",
    "                # CONCEPT = nuclei area\n",
    "                #batch_n_area.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='area'))\n",
    "                #batch_contrast.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='area'))\n",
    "                # CONCEPT = nuclei counts\n",
    "                #batch_n_count.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='count'))\n",
    "                #batch_contrast.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='count'))\n",
    "            #batch_domain=keras.utils.to_categorical(batch_domain, num_classes=6)\n",
    "            batch_x=np.asarray(batch_x, dtype=np.float32)\n",
    "            batch_y=np.asarray(batch_y, dtype=np.float32)\n",
    "            batch_cm=np.asarray(batch_contrast, dtype=np.float32) #ones(len(batch_y), dtype=np.float32)\n",
    "            #batch_cm=np.ones(len(batch_y), dtype=np.float32)\n",
    "            yield [batch_x, batch_y, batch_cm], None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=get_batch_data(data_list, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['extra17, normal/level7/centre2/patient055/node1/patches, 262, train\\n',\n",
       " 'extra17, normal/level7/centre2/patient056/node0/patches, 163, train\\n',\n",
       " 'extra17, normal/level7/centre2/patient055/node3/patches, 323, train\\n',\n",
       " 'extra17, normal/level7/centre0/patient003/node1/patches, 236, train\\n',\n",
       " 'extra17, normal/level7/centre0/patient012/node3/patches, 173, train\\n',\n",
       " 'extra17, normal/level7/centre0/patient011/node1/patches, 213, train\\n',\n",
       " 'extra17, normal/level7/centre0/patient012/node4/patches, 146, train\\n',\n",
       " 'extra17, normal/level7/centre2/patient049/node0/patches, 400, train\\n',\n",
       " 'extra17, normal/level7/centre0/patient008/node1/patches, 366, train\\n',\n",
       " 'extra17, normal/level7/centre2/patient041/node4/patches, 378, train\\n']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERR]: extra17, normal/level7/centre2/patient055/node1/patches, 262, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre2/patient056/node0/patches, 163, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre2/patient055/node3/patches, 323, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre0/patient003/node1/patches, 236, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre0/patient012/node3/patches, 173, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre0/patient011/node1/patches, 213, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre0/patient012/node4/patches, 146, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre2/patient049/node0/patches, 400, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre0/patient008/node1/patches, 366, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre2/patient041/node4/patches, 378, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre2/patient047/node1/patches, 338, norm_contrast\n",
      "[ERR]: cam16, normal/level7/centre0/patient009_Mask.tif/patches, 62, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre2/patient058/node4/patches, 385, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre2/patient048/node4/patches, 215, norm_contrast\n",
      "[ERR]: tumor_extra17, tumor/level7/centre2/patient040/node2/patches, 398, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre2/patient059/node1/patches, 398, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre2/patient054/node4/patches, 249, norm_contrast\n",
      "[ERR]: all500, tumor/level7/centre1/patient020/node4/patches, 433, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre2/patient056/node1/patches, 12, norm_contrast\n",
      "[ERR]: cam16, normal/level7/centre0/patient067_Mask.tif/patches, 14, norm_contrast\n",
      "[ERR]: cam16, tumor/level7/centre0/patient052_Mask.tif/patches, 130, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre3/patient061/node0/patches, 387, norm_contrast\n",
      "[ERR]: all500, normal/level7/centre1/patient024/node2/patches, 200, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre3/patient075/node2/patches, 171, norm_contrast\n",
      "[ERR]: all500, normal/level7/centre2/patient046/node3/patches, 52, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre3/patient078/node1/patches, 432, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre0/patient013/node1/patches, 199, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre2/patient057/node4/patches, 48, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre0/patient011/node1/patches, 214, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre1/patient030/node1/patches, 359, norm_contrast\n",
      "[ERR]: all500, tumor/level7/centre0/patient015/node2/patches, 431, norm_contrast\n",
      "[ERR]: extra17, normal/level7/centre2/patient050/node0/patches, 272, norm_contrast\n"
     ]
    }
   ],
   "source": [
    "[x,y,cm],_=train_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
