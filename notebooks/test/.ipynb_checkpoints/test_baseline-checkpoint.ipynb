{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder='/home/mara/multitask_adversarial/results/BASEL/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = '0'\n",
    "keras.backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../doc/data_shuffle.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'../../doc/data_shuffle.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "## Loading OS libraries to configure server preferences\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import setproctitle\n",
    "SERVER_NAME = 'ultrafast'\n",
    "EXPERIMENT_TYPE='test_baseline'\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "## Adding PROCESS_UC1 utilities\n",
    "sys.path.append('../../lib/TASK_2_UC1/')\n",
    "from models import *\n",
    "from util import otsu_thresholding\n",
    "from extract_xml import *\n",
    "from functions import *                   \n",
    "sys.path.append('../../lib/')\n",
    "from mlta import *\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = '0'\n",
    "keras.backend.set_session(tf.Session(config=config))\n",
    "\n",
    "verbose=1 \n",
    "\n",
    "cam16 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/cam16_500/patches.h5py',  'r', libver='latest', swmr=True)\n",
    "all500 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/all500/patches.h5py',  'r', libver='latest', swmr=True)\n",
    "extra17 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/extra17/patches.h5py',  'r', libver='latest', swmr=True)\n",
    "tumor_extra17=hd.File('/home/mara/adversarialMICCAI/data/ultrafast/1129-1155/patches.h5py', 'r', libver='latest', swmr=True)\n",
    "test2 = hd.File('/mnt/nas2/results/IntermediateResults/Camelyon/ultrafast/test_data2/patches.hdf5', 'r', libver='latest', swmr=True)\n",
    "pannuke= hd.File('/mnt/nas2/results/IntermediateResults/Camelyon/pannuke/patches_fix.hdf5', 'r', libver='latest', swmr=True)\n",
    "\n",
    "global data\n",
    "data={'cam16':cam16,'all500':all500,'extra17':extra17, 'tumor_extra17':tumor_extra17, 'test_data2': test2, 'pannuke':pannuke}\n",
    "global concept_db\n",
    "concept_db = hd.File('../../data/normalized_cmeasures/concept_values.h5py','r')\n",
    "# Note: nuclei_concepts not supported yet\n",
    "#global nuclei_concepts\n",
    "#nuclei_concepts=hd.File('/mnt/nas2/results/IntermediateResults/Mara/MICCAI2020/normalized.hdf5','r')\n",
    "\n",
    "#SYSTEM CONFIGS \n",
    "CONFIG_FILE = 'doc/config.cfg'\n",
    "COLOR = True\n",
    "global new_folder\n",
    "new_folder=folder_name=model_folder\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "seed=1\n",
    "print seed\n",
    "\n",
    "# SET PROCESS TITLE\n",
    "setproctitle.setproctitle('UC1_{}'.format(EXPERIMENT_TYPE))\n",
    "\n",
    "# SET SEED\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# DATA SPLIT CSVs \n",
    "train_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/train_shuffle.csv', 'r') # How is the encoding of .csv files ?\n",
    "val_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/val_shuffle.csv', 'r')\n",
    "test_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/test_shuffle.csv', 'r')\n",
    "train_list=train_csv.readlines()\n",
    "val_list=val_csv.readlines()\n",
    "test_list=test_csv.readlines()\n",
    "test2_csv = open('/mnt/nas2/results/IntermediateResults/Camelyon/test2_shuffle.csv', 'r')\n",
    "test2_list=test2_csv.readlines()\n",
    "test2_csv.close()\n",
    "train_csv.close()\n",
    "val_csv.close()\n",
    "test_csv.close()\n",
    "#data_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/data_shuffle.csv', 'r')\n",
    "#data_csv=open('./data/train.csv', 'r')\n",
    "data_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/pannuke/pannuke_train_shuffled.csv', 'r')\n",
    "data_list=data_csv.readlines()\n",
    "data_csv.close()\n",
    "\n",
    "# STAIN NORMALIZATION\n",
    "def get_normalizer(patch, save_folder=''):\n",
    "    normalizer = ReinhardNormalizer()\n",
    "    normalizer.fit(patch)\n",
    "    np.save('{}/normalizer'.format(save_folder),normalizer)\n",
    "    np.save('{}/normalizing_patch'.format(save_folder), patch)\n",
    "    print('Normalisers saved to disk.')\n",
    "    return normalizer\n",
    "\n",
    "def normalize_patch(patch, normalizer):\n",
    "    return np.float64(normalizer.transform(np.uint8(patch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using brightness standardization\n",
      "Normalisers saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA NORMALIZER\n",
    "global normalizer\n",
    "db_name, entry_path, patch_no = get_keys(data_list[0])\n",
    "normalization_reference_patch = data[db_name][entry_path][patch_no]\n",
    "normalizer = get_normalizer(normalization_reference_patch, save_folder=new_folder)\n",
    "\n",
    "\"\"\"\n",
    "Building baseline model \n",
    "\"\"\"\n",
    "#\n",
    "# MODEL: BASELINE\n",
    "base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "#base_model = keras.applications.resnet50.ResNet50(include_top=True, weights='imagenet')\n",
    "#base_model = pretrained(ResNet50)((224,224,3),1000)\n",
    "#base_model = wide_resnet(50, 2048)((224,224,3), (1))\n",
    "feature_output=base_model.layers[-1].output\n",
    "feature_output = keras.layers.GlobalAveragePooling2D()(feature_output)\n",
    "feature_output = Dense(2048, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01), name='finetuned_features1')(feature_output)\n",
    "feature_output = Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01), name='finetuned_features2')(feature_output)\n",
    "feature_output = Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01), name='finetuned_features3')(feature_output)\n",
    "finetuning = Dense(1,name='predictions')(feature_output)\n",
    "#regression_output = keras.layers.Dense(1, activation = keras.layers.Activation('linear'), name='concept_regressor')(feature_output)\n",
    "model = Model(input=base_model.input, output=[finetuning])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Batch generators: \n",
    "They load a patch list: a list of file names and paths. \n",
    "They use the list to create a batch of 32 samples. \n",
    "\"\"\"\n",
    "class Generator(Sequence):\n",
    "    # Class is a dataset wrapper for better training performance\n",
    "    def __init__(self, patch_list, batch_size=32):\n",
    "        #self.x, self.y = x_set, y_set\n",
    "        self.patch_list=patch_list\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples=len(patch_list)\n",
    "    def __len__(self):\n",
    "        return self.num_samples/self.batch_size\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            offset = 0\n",
    "            batch_size=self.batch_size\n",
    "            for offset in range(0,self.num_samples, self.batch_size):\n",
    "                batch_x = []\n",
    "                batch_y = []\n",
    "                #batch_ones=[]\n",
    "                #batch_noise=[]\n",
    "                batch_contrast = []\n",
    "                #batch_domain = []\n",
    "                #batch_n_area = [] # nuclei average area\n",
    "                #batch_n_count = []\n",
    "                batch_samples=self.patch_list[offset:offset+batch_size]\n",
    "                for line in batch_samples:\n",
    "                    db_name, entry_path, patch_no = get_keys(line)\n",
    "                    patch=data[db_name][entry_path][patch_no]\n",
    "                    patch=normalize_patch(patch, normalizer)\n",
    "                    patch=keras.applications.inception_v3.preprocess_input(patch) #removed bc of BNorm\n",
    "                    #patch=keras.applications.resnet50.preprocess_input(patch) \n",
    "                    label = get_class(line, entry_path) # is there a problem with get_class ?\n",
    "                    batch_x.append(patch)\n",
    "                    batch_y.append(label)\n",
    "                batch_x=np.asarray(batch_x, dtype=np.float32)\n",
    "                batch_y=np.asarray(batch_y, dtype=np.float32)\n",
    "                batch_cm=np.ones(len(batch_y), dtype=np.float32)\n",
    "            inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            batch_x = [batch_x, batch_y, batch_cm]\n",
    "            batch_y = batch_y\n",
    "            return batch_x, batch_y\n",
    "#def on_epoch_end(self):\n",
    "    #    np.random.shuffle(self.indices)\n",
    "def get_concept_measure(db_name, entry_path, patch_no, measure_type=''):\n",
    "    ### note: The measures in the file should have been scaled beforehand\n",
    "    # to have zero mean and unit std\n",
    "    \n",
    "    path=db_name+'/'+entry_path+'/'+str(patch_no)+'/'+measure_type\n",
    "    #import pdb; pdb.set_trace()\n",
    "    #print path\n",
    "    #print concept_db[path]\n",
    "    #print concept_db[path][0]\n",
    "    try:\n",
    "        cm=concept_db[path][0]\n",
    "        return cm\n",
    "    except:\n",
    "        print(\"[ERR]: {}, {}, {}, {} with path {}\".format(db_name, entry_path, patch_no, measure_type, path))\n",
    "        return 1.\n",
    "    \n",
    "def get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type=''):\n",
    "    ### note: The measures in the file should have been scaled beforehand\n",
    "    # to have zero mean and unit std\n",
    "    try:\n",
    "        cm = nuclei_concepts[db_name+'/'+entry_path+'/'+str(patch_no)+'/'+measure_type][0]\n",
    "    except:\n",
    "        error_log.write('[get_segmented_concept_measure] {}, {}, {}, {}'.format(db_name, entry_path, patch_no, measure_type))\n",
    "        print \"[ERROR] Issue retreiving concept measure for {}, {}, {}, {}\".format(db_name, entry_path, patch_no, measure_type)\n",
    "        return 1.\n",
    "\n",
    "# BATCH GENERATORS\n",
    "def get_batch_data(patch_list, batch_size=32):\n",
    "    num_samples=len(patch_list)\n",
    "    while True:\n",
    "        offset = 0\n",
    "        for offset in range(0,num_samples, batch_size):\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            batch_contrast=[]\n",
    "            batch_samples=patch_list[offset:offset+batch_size]\n",
    "            for line in batch_samples[:(num_samples//batch_size)*batch_size]:\n",
    "                db_name, entry_path, patch_no = get_keys(line)\n",
    "                patch=data[db_name][entry_path][patch_no]\n",
    "                patch=normalize_patch(patch, normalizer)\n",
    "                patch=keras.applications.inception_v3.preprocess_input(patch) \n",
    "                label = get_class(line, entry_path) \n",
    "                batch_x.append(patch)\n",
    "                batch_y.append(label)\n",
    "                # ONES\n",
    "                #batch_ones.append(1.)\n",
    "                # NOISE\n",
    "                #batch_noise.append(np.random.normal(0.))\n",
    "                # CONCEPT = contrast\n",
    "                batch_contrast.append(get_concept_measure(db_name, entry_path, patch_no, measure_type='norm_contrast'))\n",
    "                # CONCEPT = domain\n",
    "                #batch_domain.append(get_domain(db_name, entry_path))\n",
    "                # CONCEPT = nuclei area\n",
    "                #batch_n_area.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='area'))\n",
    "                #batch_contrast.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='area'))\n",
    "                # CONCEPT = nuclei counts\n",
    "                #batch_n_count.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='count'))\n",
    "                #batch_contrast.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='count'))\n",
    "            #batch_domain=keras.utils.to_categorical(batch_domain, num_classes=6)\n",
    "            batch_x=np.asarray(batch_x, dtype=np.float32)\n",
    "            batch_y=np.asarray(batch_y, dtype=np.float32)\n",
    "            batch_cm=np.asarray(batch_contrast, dtype=np.float32) #ones(len(batch_y), dtype=np.float32)\n",
    "            #batch_cm=np.ones(len(batch_y), dtype=np.float32)\n",
    "            yield batch_x, batch_y#, batch_cm\n",
    "            \n",
    "def get_test_batch(patch_list, batch_size=32):\n",
    "    num_samples=len(patch_list)\n",
    "    while True:     \n",
    "        for offset in range(0,num_samples, batch_size):\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            batch_contrast = []\n",
    "            batch_samples=patch_list[offset:offset+batch_size]\n",
    "            for line in batch_samples:\n",
    "                db_name, entry_path, patch_no = get_keys(line)\n",
    "                \n",
    "                patch=data[db_name][entry_path][patch_no]\n",
    "                patch=normalize_patch(patch, normalizer)\n",
    "                patch=keras.applications.inception_v3.preprocess_input(patch)\n",
    "                #patch=keras.applications.resnet50.preprocess_input(patch)\n",
    "                label = get_test_label(entry_path)\n",
    "                #print db_name, entry_path, patch_no, label\n",
    "                batch_x.append(patch)\n",
    "                batch_y.append(label)\n",
    "                #batch_ones.append(1.)\n",
    "                # NOISE\n",
    "                #batch_noise.append(np.random.normal(0.))\n",
    "                # CONCEPT = contrast\n",
    "                batch_contrast.append(get_concept_measure(db_name, entry_path, patch_no, measure_type='norm_contrast'))\n",
    "                # CONCEPT = domain\n",
    "                #batch_domain.append(get_domain(db_name, entry_path))\n",
    "                # CONCEPT = nuclei area\n",
    "                #batch_n_area.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='area'))\n",
    "                #batch_contrast.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='area'))\n",
    "                # CONCEPT = nuclei counts\n",
    "                #batch_n_count.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='count'))\n",
    "                #batch_contrast.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='count'))\n",
    "            #batch_domain=keras.utils.to_categorical(batch_domain, num_classes=6)\n",
    "            batch_x=np.asarray(batch_x, dtype=np.float32)\n",
    "            batch_y=np.asarray(batch_y, dtype=np.float32)\n",
    "            #batch_cm=np.ones(len(batch_y), dtype=np.float32)\n",
    "            batch_cm=np.asarray(batch_contrast, dtype=np.float32) # np.ones(len(batch_y), dtype=np.float32)\n",
    "            yield batch_x, batch_y#, batch_cm\n",
    "            #yield np.asarray(batch_x, dtype=np.float32), np.asarray(batch_y, dtype=np.float32), np.ones(len(batch_y), dtype=np.float32)#, np.asarray(batch_cm, dtype=np.float32)\n",
    "\n",
    "\n",
    "# In[2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(d_list, model, batch_size=BATCH_SIZE):\n",
    "    test_generator=get_test_batch(d_list, batch_size=batch_size)\n",
    "    steps=len(d_list)//batch_size\n",
    "    print steps\n",
    "    initial_lr = 1e-4\n",
    "    opt = keras.optimizers.SGD(lr=initial_lr, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=opt, \n",
    "                 loss= [classifier_loss],\n",
    "                   metrics= [my_accuracy])\n",
    "    callbacks = []\n",
    "    y_true=np.zeros(len(d_list))\n",
    "    y_pred=np.zeros((len(d_list),1))\n",
    "    N=0\n",
    "    while N<len(d_list):\n",
    "        x_b, y_b = test_generator.next()\n",
    "        y_p_b = model.predict(x_b)\n",
    "        y_true[N:N+len(y_b)]=y_b\n",
    "        y_pred[N:N+len(y_p_b)]=y_p_b\n",
    "        N+=len(y_p_b)\n",
    "    sliced_y_pred = tf.sigmoid(y_pred)\n",
    "    y_pred_rounded = K.round(sliced_y_pred)\n",
    "    print 'accuracy: ', accuracy_score(y_pred_rounded.eval(session=tf.Session()), y_true)\n",
    "    sliced_y_pred=sliced_y_pred.eval(session=tf.Session())\n",
    "    auc_score=sklearn.metrics.roc_auc_score(y_true,sliced_y_pred)\n",
    "    print 'auc: ', auc_score\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    y_true=np.reshape(y_true,(len(y_true),1))\n",
    "    for i in range(1):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], sliced_y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), sliced_y_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc_score\n",
    "    plot=False\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "        plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "                 lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic example')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "    return auc_score, y_pred_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_folder+'/best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "accuracy:  0.7019386106623586\n",
      "auc:  0.7870614607595342\n"
     ]
    }
   ],
   "source": [
    "list_=test_list+test2_list\n",
    "auc_score, y_pred_rounded=evaluate_model(list_,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "accuracy:  0.7035541195476576\n",
      "auc:  0.7843508679851962\n",
      "116\n",
      "accuracy:  0.6976305869682283\n",
      "auc:  0.7909975857989268\n",
      "116\n",
      "accuracy:  0.7272482498653743\n",
      "auc:  0.8034849567935647\n",
      "116\n",
      "accuracy:  0.7011308562197092\n",
      "auc:  0.7867221637687483\n",
      "116\n",
      "accuracy:  0.7113624124932687\n",
      "auc:  0.7956770550891037\n",
      "116\n",
      "accuracy:  0.7094776521270867\n",
      "auc:  0.7923959890995591\n",
      "116\n",
      "accuracy:  0.7035541195476576\n",
      "auc:  0.7892464102295357\n",
      "116\n",
      "accuracy:  0.7014001077005924\n",
      "auc:  0.7832763785052101\n",
      "116\n",
      "accuracy:  0.7089391491653204\n",
      "auc:  0.7871577059040785\n",
      "116\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a1697bc5a376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtest_list_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_bootstrap_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mroc_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_list_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, test_type='bootstrap_overall')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0maucs_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"AUC avg (std): {} ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maucs_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maucs_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-20f3604b2230>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(d_list, model, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0my_p_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-4e6f63710bf5>\u001b[0m in \u001b[0;36mget_test_batch\u001b[0;34m(patch_list, batch_size)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mpatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdb_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentry_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatch_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0mpatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0mpatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;31m#patch=keras.applications.resnet50.preprocess_input(patch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9459800835f9>\u001b[0m in \u001b[0;36mnormalize_patch\u001b[0;34m(patch, normalizer)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/mara/multitask_adversarial/lib/TASK_2_UC1/normalizers.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, I)\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize_brightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mI1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlab_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_std\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0mnorm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_stds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_means\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mnorm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_stds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_means\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/multitask_adversarial/lib/TASK_2_UC1/normalizers.pyc\u001b[0m in \u001b[0;36mget_mean_std\u001b[0;34m(self, I)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \"\"\"\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0mI1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlab_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msd1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeanStdDev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msd2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeanStdDev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/multitask_adversarial/lib/TASK_2_UC1/normalizers.pyc\u001b[0m in \u001b[0;36mlab_split\u001b[0;34m(I)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_RGB2LAB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mI1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0mI1\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m2.55\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mI2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m128.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_bootstrap_sample(data, n_samples=2):\n",
    "    sample_=[data[i] for i in np.random.choice(len(data),n_samples)]\n",
    "    #sample_=[data[i] for i in range(len(data))]\n",
    "    return sample_\n",
    "\n",
    "keras.backend.get_session().run(tf.global_variables_initializer())\n",
    "model.load_weights(model_folder+'/best_model.h5')\n",
    "aucs_i=[]\n",
    "for i in range(50):\n",
    "    test_list_b=get_bootstrap_sample(list_, n_samples=len(list_))\n",
    "    roc_auc, _=evaluate_model(test_list_b,model)#, test_type='bootstrap_overall')\n",
    "    aucs_i.append(roc_auc)\n",
    "print \"AUC avg (std): {} ({})\".format(np.mean(aucs_i), np.std(aucs_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aucs_i\n",
    "guided_aucs=[0.8097256593007709,\n",
    " 0.8257677583461371,\n",
    " 0.8284987543793056,\n",
    " 0.8123987690905612,\n",
    " 0.8122129193557766,\n",
    " 0.8171528762435989,\n",
    " 0.8203942669480053,\n",
    " 0.801909346122524,\n",
    " 0.8110533838973162,\n",
    " 0.8192357512953368,\n",
    " 0.8163210561918313,\n",
    " 0.8049622015028518,\n",
    " 0.8164248807485958,\n",
    " 0.8261588579545603,\n",
    " 0.8224349505654979,\n",
    " 0.8198083365432877,\n",
    " 0.8233338473508035,\n",
    " 0.8151898394372206,\n",
    " 0.8116935267976655,\n",
    " 0.8230061920156451,\n",
    " 0.80999705999706,\n",
    " 0.8225011543919973,\n",
    " 0.8194816813770406,\n",
    " 0.8124183815864469,\n",
    " 0.8216119126421779,\n",
    " 0.8131742798952575,\n",
    " 0.8133916206624894,\n",
    " 0.8185327317394306,\n",
    " 0.8132800425609821,\n",
    " 0.826003441176462,\n",
    " 0.81668630997849,\n",
    " 0.8185735954190274,\n",
    " 0.8031802696797476,\n",
    " 0.8128920345130592,\n",
    " 0.8152530177337852,\n",
    " 0.8212378998659381,\n",
    " 0.8154698586741791,\n",
    " 0.8117851422186724,\n",
    " 0.8090521202113915,\n",
    " 0.8107352601298298,\n",
    " 0.812407687849384,\n",
    " 0.8090276887279646,\n",
    " 0.8221827009936765,\n",
    " 0.8134452910776464,\n",
    " 0.8137599048373841,\n",
    " 0.8127480241741258,\n",
    " 0.812849532383189,\n",
    " 0.8179884057229148,\n",
    " 0.8306913580318795,\n",
    " 0.8171719576042143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_ind(aucs_i, guided_aucs, equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.wilcoxon(aucs_i, guided_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.mannwhitneyu(aucs_i, guided_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score, y_pred_rounded=evaluate_model(test_list,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(test2_list,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data_(patch_list, batch_size=32):\n",
    "    print batch_size\n",
    "    num_samples=len(patch_list)\n",
    "    while True:\n",
    "        offset = 0\n",
    "        for offset in range(0,num_samples, batch_size):\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            batch_samples=patch_list[offset:offset+batch_size]\n",
    "            for line in batch_samples[:(num_samples//batch_size)*batch_size]:\n",
    "                print line\n",
    "                db_name, entry_path, patch_no = get_keys(line)\n",
    "                patch=data[db_name][entry_path][patch_no]\n",
    "                patch=normalize_patch(patch, normalizer)\n",
    "                patch=keras.applications.inception_v3.preprocess_input(patch) \n",
    "                label = get_class(line, entry_path) \n",
    "                batch_x.append(patch)\n",
    "                batch_y.append(label)\n",
    "            batch_x=np.asarray(batch_x, dtype=np.float32)\n",
    "            batch_y=np.asarray(batch_y, dtype=np.float32)\n",
    "            generator_output=[batch_x, batch_y]\n",
    "            \n",
    "            for c in CONCEPT:\n",
    "                batch_concept_values=[]\n",
    "                for line in batch_samples[:(num_samples//batch_size)*batch_size]:\n",
    "                    #print 'concept: ', c, line\n",
    "                    db_name, entry_path, patch_no = get_keys(line)\n",
    "                    batch_concept_values.append(get_concept_measure(db_name, entry_path, patch_no, measure_type=c))\n",
    "                batch_concept_values=np.asarray(batch_concept_values, dtype=np.float32)\n",
    "                generator_output.append(batch_concept_values)\n",
    "            yield generator_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCEPT=['ncount', 'narea', 'full_contrast']#, 'domain']\n",
    "error_log=open('./baseline_test_log.txt', 'a')\n",
    "test_generator = get_batch_data_(test2_list, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "test_data2, normal/level7/centre4/patient081/node4/patches, 360\n",
      "\n",
      "concept:  ncount test_data2, normal/level7/centre4/patient081/node4/patches, 360\n",
      "\n",
      "concept:  narea test_data2, normal/level7/centre4/patient081/node4/patches, 360\n",
      "\n",
      "concept:  full_contrast test_data2, normal/level7/centre4/patient081/node4/patches, 360\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[[[ 0.8509804 ,  0.12941177,  0.69411767],\n",
       "           [ 0.85882354,  0.18431373,  0.69411767],\n",
       "           [ 0.85882354,  0.22352941,  0.69411767],\n",
       "           ...,\n",
       "           [ 0.23137255, -0.6313726 , -0.15294118],\n",
       "           [ 0.3647059 , -0.49803922, -0.03529412],\n",
       "           [ 0.49803922, -0.39607844,  0.07450981]],\n",
       "  \n",
       "          [[ 0.7882353 ,  0.08235294,  0.7176471 ],\n",
       "           [ 0.8039216 ,  0.10588235,  0.70980394],\n",
       "           [ 0.8352941 ,  0.15294118,  0.7019608 ],\n",
       "           ...,\n",
       "           [ 0.31764707, -0.5764706 , -0.1764706 ],\n",
       "           [ 0.35686275, -0.54509807, -0.16862746],\n",
       "           [ 0.3882353 , -0.5294118 , -0.12156863]],\n",
       "  \n",
       "          [[ 0.7019608 , -0.02745098,  0.77254903],\n",
       "           [ 0.7411765 ,  0.00392157,  0.7411765 ],\n",
       "           [ 0.70980394, -0.01176471,  0.75686276],\n",
       "           ...,\n",
       "           [ 0.42745098, -0.41960785, -0.13725491],\n",
       "           [ 0.37254903, -0.54509807, -0.21568628],\n",
       "           [ 0.3019608 , -0.6156863 , -0.2784314 ]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[-0.12941177, -1.        , -0.7254902 ],\n",
       "           [-0.13725491, -1.        , -0.7882353 ],\n",
       "           [-0.11372549, -1.        , -0.7647059 ],\n",
       "           ...,\n",
       "           [ 0.8039216 ,  0.09803922,  0.31764707],\n",
       "           [ 0.75686276,  0.06666667,  0.2784314 ],\n",
       "           [ 0.75686276,  0.06666667,  0.2784314 ]],\n",
       "  \n",
       "          [[ 0.16862746, -0.7176471 , -0.5137255 ],\n",
       "           [ 0.06666667, -0.9137255 , -0.6313726 ],\n",
       "           [-0.01960784, -1.        , -0.7254902 ],\n",
       "           ...,\n",
       "           [ 0.78039217,  0.05098039,  0.2784314 ],\n",
       "           [ 0.75686276,  0.05098039,  0.20784314],\n",
       "           [ 0.78039217,  0.07450981,  0.23137255]],\n",
       "  \n",
       "          [[ 0.54509807, -0.31764707, -0.22352941],\n",
       "           [ 0.37254903, -0.49803922, -0.42745098],\n",
       "           [ 0.15294118, -0.7254902 , -0.6       ],\n",
       "           ...,\n",
       "           [ 0.64705884, -0.05882353,  0.19215687],\n",
       "           [ 0.7411765 ,  0.03529412,  0.19215687],\n",
       "           [ 0.79607844,  0.08235294,  0.23921569]]]], dtype=float32),\n",
       "  array([0.], dtype=float32),\n",
       "  array([0.5717621], dtype=float32),\n",
       "  array([0.07420972], dtype=float32),\n",
       "  array([-0.8194073], dtype=float32)],\n",
       " None)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rsquared(labels, predictions):\n",
    "    errors = labels - predictions\n",
    "    sum_squared_errors = np.sum(np.asarray([pow(errors[i],2) for i in range(len(errors))]))\n",
    "    # total sum of squares, TTS\n",
    "    average_y = np.mean(labels)\n",
    "    total_errors = labels - average_y\n",
    "    total_sum_squares = np.sum(np.asarray([pow(total_errors[i],2) for i in range(len(total_errors))]))\n",
    "    #rsquared is 1-RSS/TTS\n",
    "    rss_over_tts =   sum_squared_errors/total_sum_squares\n",
    "    rsquared = 1-rss_over_tts\n",
    "    return rsquared\n",
    "def compute_mse(labels, predictions):\n",
    "    errors = labels - predictions\n",
    "    sum_squared_errors = np.sum(np.asarray([pow(errors[i],2) for i in range(len(errors))]))\n",
    "    mse = sum_squared_errors / len(labels)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_i = compute_rsquared(all_cm_i, all_p_cm_i)\n",
    "mse_i = compute_mse(all_cm_i, all_p_cm_i)\n",
    "print 'Internal: ', r2_i, mse_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_e = compute_rsquared(all_cm_e, all_p_cm_e)\n",
    "mse_e = compute_mse(all_cm_e, all_p_cm_e)\n",
    "print 'External: ', r2_e, mse_e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
