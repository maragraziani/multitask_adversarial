{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder='/home/mara/multitask_adversarial/results/shallowCNN/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Using brightness standardization\n",
      "Normalisers saved to disk.\n"
     ]
    }
   ],
   "source": [
    "## Loading OS libraries to configure server preferences\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import setproctitle\n",
    "SERVER_NAME = 'ultrafast'\n",
    "EXPERIMENT_TYPE='test_baseline'\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "## Adding PROCESS_UC1 utilities\n",
    "sys.path.append('/home/mara/multitask_adversarial/lib/TASK_2_UC1/')\n",
    "from models import *\n",
    "from util import otsu_thresholding\n",
    "from extract_xml import *\n",
    "from functions import *                   \n",
    "sys.path.append('/home/mara/multitask_adversarial/lib/')\n",
    "from mlta import *\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import keras\n",
    "import sys\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.optimizers import SGD, Adadelta\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = '0'\n",
    "keras.backend.set_session(tf.Session(config=config))\n",
    "\n",
    "verbose=1 \n",
    "\n",
    "cam16 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/cam16_500/patches.hdf5',  'r', libver='latest', swmr=True)\n",
    "all500 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/all500/patches.hdf5',  'r', libver='latest', swmr=True)\n",
    "extra17 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/extra17/patches.hdf5',  'r', libver='latest', swmr=True)\n",
    "tumor_extra17=hd.File('/home/mara/adversarialMICCAI/data/ultrafast/1129-1155/patches.hdf5', 'r', libver='latest', swmr=True)\n",
    "test2 = hd.File('/mnt/nas2/results/IntermediateResults/Camelyon/ultrafast/test_data2/patches.hdf5', 'r', libver='latest', swmr=True)\n",
    "pannuke= hd.File('/mnt/nas2/results/IntermediateResults/Camelyon/pannuke/patches_fix.hdf5', 'r', libver='latest', swmr=True)\n",
    "\n",
    "global data\n",
    "data={'cam16':cam16,'all500':all500,'extra17':extra17, 'tumor_extra17':tumor_extra17, 'test_data2': test2, 'pannuke':pannuke}\n",
    "global concept_db\n",
    "concept_db = hd.File('/mnt/nas2/results/IntermediateResults/Mara/MICCAI2020/MELBA_normalized_concepts.hd', 'r')\n",
    "# Note: nuclei_concepts not supported yet\n",
    "global nuclei_concepts\n",
    "nuclei_concepts=hd.File('/mnt/nas2/results/IntermediateResults/Mara/MICCAI2020/normalized_nuclei_concepts_db_new_try_def.hdf5','r')\n",
    "\n",
    "#SYSTEM CONFIGS \n",
    "CONFIG_FILE = 'doc/config.cfg'\n",
    "COLOR = True\n",
    "global new_folder\n",
    "new_folder=folder_name=model_folder\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "seed=1\n",
    "print seed\n",
    "\n",
    "# SET PROCESS TITLE\n",
    "setproctitle.setproctitle('UC1_{}'.format(EXPERIMENT_TYPE))\n",
    "\n",
    "# SET SEED\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# DATA SPLIT CSVs \n",
    "train_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/train_shuffle.csv', 'r') # How is the encoding of .csv files ?\n",
    "val_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/val_shuffle.csv', 'r')\n",
    "test_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/test_shuffle.csv', 'r')\n",
    "train_list=train_csv.readlines()\n",
    "val_list=val_csv.readlines()\n",
    "test_list=test_csv.readlines()\n",
    "test2_csv = open('/mnt/nas2/results/IntermediateResults/Camelyon/test2_shuffle.csv', 'r')\n",
    "test2_list=test2_csv.readlines()\n",
    "test2_csv.close()\n",
    "train_csv.close()\n",
    "val_csv.close()\n",
    "test_csv.close()\n",
    "#data_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/data_shuffle.csv', 'r')\n",
    "#data_csv=open('./data/train.csv', 'r')\n",
    "data_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/pannuke/pannuke_train_shuffled.csv', 'r')\n",
    "data_list=data_csv.readlines()\n",
    "data_csv.close()\n",
    "\n",
    "# STAIN NORMALIZATION\n",
    "def get_normalizer(patch, save_folder=''):\n",
    "    normalizer = ReinhardNormalizer()\n",
    "    normalizer.fit(patch)\n",
    "    np.save('{}/normalizer'.format(save_folder),normalizer)\n",
    "    np.save('{}/normalizing_patch'.format(save_folder), patch)\n",
    "    print('Normalisers saved to disk.')\n",
    "    return normalizer\n",
    "\n",
    "def normalize_patch(patch, normalizer):\n",
    "    return np.float64(normalizer.transform(np.uint8(patch)))\n",
    "global normalizer\n",
    "db_name, entry_path, patch_no = get_keys(data_list[0])\n",
    "normalization_reference_patch = data[db_name][entry_path][patch_no]\n",
    "normalizer = get_normalizer(normalization_reference_patch, save_folder=new_folder)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Batch generators: \n",
    "They load a patch list: a list of file names and paths. \n",
    "They use the list to create a batch of 32 samples. \n",
    "\"\"\"\n",
    "class Generator(Sequence):\n",
    "    # Class is a dataset wrapper for better training performance\n",
    "    def __init__(self, patch_list, batch_size=32):\n",
    "        #self.x, self.y = x_set, y_set\n",
    "        self.patch_list=patch_list\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples=len(patch_list)\n",
    "    def __len__(self):\n",
    "        return self.num_samples/self.batch_size\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            offset = 0\n",
    "            batch_size=self.batch_size\n",
    "            for offset in range(0,self.num_samples, self.batch_size):\n",
    "                batch_x = []\n",
    "                batch_y = []\n",
    "                #batch_ones=[]\n",
    "                #batch_noise=[]\n",
    "                batch_contrast = []\n",
    "                #batch_domain = []\n",
    "                #batch_n_area = [] # nuclei average area\n",
    "                batch_n_count = []\n",
    "                batch_samples=self.patch_list[offset:offset+batch_size]\n",
    "                for line in batch_samples:\n",
    "                    db_name, entry_path, patch_no = get_keys(line)\n",
    "                    patch=data[db_name][entry_path][patch_no]\n",
    "                    patch=normalize_patch(patch, normalizer)\n",
    "                    patch=keras.applications.inception_v3.preprocess_input(patch) #removed bc of BNorm\n",
    "                    #patch=keras.applications.resnet50.preprocess_input(patch) \n",
    "                    label = get_class(line, entry_path) # is there a problem with get_class ?\n",
    "                    batch_x.append(patch)\n",
    "                    batch_y.append(label)\n",
    "                batch_x=np.asarray(batch_x, dtype=np.float32)\n",
    "                batch_y=np.asarray(batch_y, dtype=np.float32)\n",
    "                batch_cm=np.ones(len(batch_y), dtype=np.float32)\n",
    "            inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            batch_x = [batch_x, batch_y, batch_cm]\n",
    "            batch_y = batch_y\n",
    "            return batch_x, batch_y\n",
    "\n",
    "    #def on_epoch_end(self):\n",
    "    #    np.random.shuffle(self.indices)\n",
    "\n",
    "# Retrieve Concept Measures\n",
    "def get_concept_measure(db_name, entry_path, patch_no, measure_type=''):\n",
    "    ### note: The measures in the file should have been scaled beforehand\n",
    "    # to have zero mean and unit std\n",
    "    if db_name=='pannuke':\n",
    "        #import pdb; pdb.set_trace()\n",
    "        try:\n",
    "            cm=concept_db[entry_path+'  /'+measure_type][0]\n",
    "            return cm\n",
    "        except:\n",
    "            print \"[ERR]: {}, {}, {}, {}\".format(db_name, entry_path, patch_no, measure_type)\n",
    "            return 1.\n",
    "    else:\n",
    "        try: \n",
    "            cm=concept_db[db_name+'/'+entry_path+'/'+str(patch_no)+'/'+measure_type][0]\n",
    "            return cm\n",
    "        except:\n",
    "            print \"[ERR]: {}, {}, {}, {}\".format(db_name, entry_path, patch_no, measure_type)\n",
    "            error_log.write('[get_concept_measure] {}, {}, {}, {}'.format(db_name, entry_path, patch_no, measure_type))\n",
    "            return 1.\n",
    "def get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type=''):\n",
    "    ### note: The measures in the file should have been scaled beforehand\n",
    "    # to have zero mean and unit std\n",
    "    try:\n",
    "        cm = nuclei_concepts[db_name+'/'+entry_path+'/'+str(patch_no)+'/'+measure_type][0]\n",
    "        return cm\n",
    "    except:\n",
    "        print \"[ERROR] Error with [get_segmented_concept_measure] {}, {}, {}, {}\".format(db_name, entry_path, patch_no, measure_type) \n",
    "        #error_log.write('[get_segmented_concept_measure] {}, {}, {}, {}'.format(db_name, entry_path, patch_no, measure_type))\n",
    "        return 1.\n",
    "    \n",
    "# BATCH GENERATORS\n",
    "def get_batch_data(patch_list, batch_size=32):\n",
    "    num_samples=len(patch_list)\n",
    "    while True:\n",
    "        offset = 0\n",
    "        for offset in range(0,num_samples, batch_size):\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            batch_contrast=[]\n",
    "            batch_n_count=[]\n",
    "            batch_samples=patch_list[offset:offset+batch_size]\n",
    "            for line in batch_samples:\n",
    "                db_name, entry_path, patch_no = get_keys(line)\n",
    "                patch=data[db_name][entry_path][patch_no]\n",
    "                patch=normalize_patch(patch, normalizer)\n",
    "                patch=keras.applications.inception_v3.preprocess_input(patch) #removed bc of BNorm\n",
    "                #patch=keras.applications.resnet50.preprocess_input(patch) \n",
    "                label = get_class(line, entry_path) # is there a problem with get_class ?\n",
    "                batch_x.append(patch)\n",
    "                batch_y.append(label)\n",
    "                # ONES\n",
    "                #batch_ones.append(1.)\n",
    "                # NOISE\n",
    "                #batch_noise.append(np.random.normal(0.))\n",
    "                # CONCEPT = contrast\n",
    "                #batch_contrast.append(get_concept_measure(db_name, entry_path, patch_no, measure_type='norm_contrast'))\n",
    "                # CONCEPT = domain\n",
    "                #batch_domain.append(get_domain(db_name, entry_path))\n",
    "                # CONCEPT = nuclei area\n",
    "                #batch_n_area.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='area'))\n",
    "                # CONCEPT = nuclei counts\n",
    "                #batch_n_count.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='count'))\n",
    "            #batch_domain=keras.utils.to_categorical(batch_domain, num_classes=6)\n",
    "            batch_x=np.asarray(batch_x, dtype=np.float32)\n",
    "            batch_y=np.asarray(batch_y, dtype=np.float32)\n",
    "            #batch_cm=np.asarray(batch_contrast, dtype=np.float32) #ones(len(batch_y), dtype=np.float32)\n",
    "            yield batch_x, batch_y#, batch_cm], None\n",
    "            \n",
    "def get_test_batch(patch_list, batch_size=32):\n",
    "    num_samples=len(patch_list)\n",
    "    while True:     \n",
    "        for offset in range(0,num_samples, batch_size):\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            batch_contrast= []\n",
    "            batch_samples=patch_list[offset:offset+batch_size]\n",
    "            for line in batch_samples:\n",
    "                db_name, entry_path, patch_no = get_keys(line)\n",
    "                patch=data[db_name][entry_path][patch_no]\n",
    "                patch=normalize_patch(patch, normalizer)\n",
    "                patch=keras.applications.inception_v3.preprocess_input(patch)\n",
    "                #patch=keras.applications.resnet50.preprocess_input(patch)\n",
    "                label = get_test_label(entry_path)\n",
    "                batch_x.append(patch)\n",
    "                batch_y.append(label)# ONES\n",
    "                #batch_ones.append(1.)\n",
    "                # NOISE\n",
    "                #batch_noise.append(np.random.normal(0.))\n",
    "                # CONCEPT = contrast\n",
    "                #batch_contrast.append(get_concept_measure(db_name, entry_path, patch_no, measure_type='norm_contrast'))\n",
    "                # CONCEPT = domain\n",
    "                #batch_domain.append(get_domain(db_name, entry_path))\n",
    "                # CONCEPT = nuclei area\n",
    "                #batch_n_area.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='area'))\n",
    "                # CONCEPT = nuclei counts\n",
    "                #batch_contrast.append(get_segmented_concept_measure(db_name, entry_path, patch_no, measure_type='count'))\n",
    "            #batch_domain=keras.utils.to_categorical(batch_domain, num_classes=6)\n",
    "            batch_x=np.asarray(batch_x, dtype=np.float32)\n",
    "            batch_y=np.asarray(batch_y, dtype=np.float32)\n",
    "            #batch_cm=np.asarray(batch_contrast, dtype=np.float32) # np.ones(len(batch_y), dtype=np.float32)\n",
    "            yield batch_x, batch_y #, batch_cm], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 32)      2432      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 224, 224, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 64)      51264     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               51380480  \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 51,434,433\n",
      "Trainable params: 51,434,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "nb layers: 10\n"
     ]
    }
   ],
   "source": [
    "#logging.info(\"building model\")\n",
    "input_shape=(224,224,3)\n",
    "nb_epochs = 15 \n",
    "batch_size = 32 \n",
    "nb_dense_layers = 256 \n",
    "verbose = 2 \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st conv => relu => pool\n",
    "model.add(Conv2D(32, kernel_size=(5,5), padding=\"same\", input_shape=input_shape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# 2nd conv => relu => pool\n",
    "model.add(Conv2D(64, kernel_size=(5,5), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# flatten => relu layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(nb_dense_layers))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# final binary layer \n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# compile and display model\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(\"nb layers: \"+str(len(model.layers)))\n",
    "\n",
    "# use checkpointing to save best weights\n",
    "checkpoint = ModelCheckpoint(\"{}pcam_weights.hd5\".format(new_folder), monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('{}/weights.h5'.format(new_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need more than 2 values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3c85384c0fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mx_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0my_p_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need more than 2 values to unpack"
     ]
    }
   ],
   "source": [
    "test_generator=get_test_batch(test_list, batch_size=BATCH_SIZE)\n",
    "steps=len(test_list)//BATCH_SIZE\n",
    "print steps\n",
    "initial_lr = 1e-4\n",
    "opt = keras.optimizers.SGD(lr=initial_lr, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=opt, \n",
    "             loss= [classifier_loss],\n",
    "               metrics= [my_accuracy])\n",
    "callbacks = []\n",
    "y_true=np.zeros(len(test_list))\n",
    "y_pred=np.zeros((len(test_list),1))\n",
    "N=0\n",
    "while N<len(test_list):\n",
    "    x_b, y_b = test_generator.next()\n",
    "    y_p_b = model.predict(x_b)\n",
    "    y_true[N:N+len(y_b)]=y_b\n",
    "    y_pred[N:N+len(y_p_b)]=y_p_b\n",
    "    N+=len(y_p_b)\n",
    "y_true=y_true.reshape((len(test_list),1))\n",
    "acc = my_accuracy(y_true, y_pred).eval(session=tf.Session())\n",
    "from sklearn.metrics import accuracy_score\n",
    "sliced_y_pred = tf.sigmoid(y_pred)\n",
    "y_pred_rounded = K.round(sliced_y_pred)\n",
    "accuracy_score(y_pred_rounded.eval(session=tf.Session()), y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_accuracy(y_true, y_pred):\n",
    "    sliced_y_pred = tf.sigmoid(y_pred)\n",
    "    y_pred_rounded = K.round(sliced_y_pred)\n",
    "    acc = tf.equal(y_pred_rounded, y_true)\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "    return acc\n",
    "avg_auc = 0\n",
    "T_B=len(test_list)//BATCH_SIZE\n",
    "ys=np.zeros(len(test_list))\n",
    "preds=np.zeros((len(test_list),1))\n",
    "for i in range(T_B):\n",
    "    x,y,c=test_generator.next()\n",
    "    ys[i*BATCH_SIZE:(i)*BATCH_SIZE+len(y)] = y\n",
    "    preds[i*BATCH_SIZE:(i)*BATCH_SIZE+len(y)] = model.predict(x)\n",
    "from sklearn.metrics import auc\n",
    "auc_score=sklearn.metrics.roc_auc_score(ys,preds)\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(1):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc_score\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_true, y_pred_rounded.eval(session=tf.Session()))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_test_generator=get_test_batch(test2_list, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "steps=len(test2_list)//BATCH_SIZE\n",
    "print(steps)\n",
    "initial_lr = 1e-4\n",
    "opt = keras.optimizers.SGD(lr=initial_lr, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=opt, #optimizers.SGD(lr=1e-4, decay=1e-6, momentum=0.9, nesterov=True),\n",
    "             loss= None, #[classifier_loss, 'mean_squared_error'],\n",
    "               metrics=None)\n",
    "callbacks = []\n",
    "y_true=np.zeros(len(test2_list))\n",
    "y_pred=np.zeros((len(test2_list),1))\n",
    "N=0\n",
    "while N<len(test2_list):\n",
    "    x_b, y_b, cm_b = external_test_generator.next()\n",
    "    pred_ = model.predict(x_b)\n",
    "    y_p_b = pred_[:,0]\n",
    "    #cm_p_b = pred_[:,3]\n",
    "    y_true[N:N+len(y_b)]=y_b\n",
    "    y_pred[N:N+len(y_p_b)]=y_p_b.reshape(len(y_b),1)\n",
    "    N+=len(y_p_b)\n",
    "y_true=y_true.reshape((len(test2_list),1))\n",
    "acc = my_accuracy(y_true, y_pred).eval(session=tf.Session())\n",
    "sliced_y_pred = tf.sigmoid(y_pred)\n",
    "y_pred_rounded = K.round(sliced_y_pred)\n",
    "acc_sc = accuracy_score(y_pred_rounded.eval(session=tf.Session()), y_true)\n",
    "print('accuracy: ', acc_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "auc_score=sklearn.metrics.roc_auc_score(y_true,sliced_y_pred.eval(session=tf.Session()))\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(1):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc_score\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
