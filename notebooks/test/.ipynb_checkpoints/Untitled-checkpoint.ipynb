{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(0)# str(hvd.local_rank())\n",
    "keras.backend.set_session(tf.Session(config=config))\n",
    "verbose=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concept random ['random']\n",
      "1\n",
      "Using brightness standardization\n",
      "in\n",
      "training\n",
      "phase 1\n",
      "conv2d_88\n",
      "conv2d_89\n",
      "conv2d_92\n",
      "conv2d_93\n",
      "conv2d_86\n",
      "1\n",
      "extra_0\n",
      "1\n",
      "extra_0\n",
      "116\n",
      "('accuracy: ', 0.7444803446418955)\n",
      "auc: 0.817293778084\n",
      "0.8172937780843774\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RUN SEQUENTIAL\n",
    "python hvd_train_unc.py SEED EXPERIMENT_TYPE \n",
    "\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "from keras import *\n",
    "import setproctitle\n",
    "SERVER_NAME = 'ultrafast'\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "sys.path.append('../../lib/TASK_2_UC1/')\n",
    "from models import *\n",
    "from util import otsu_thresholding\n",
    "from extract_xml import *\n",
    "from functions import *                   \n",
    "sys.path.append('../../lib/')\n",
    "from mlta import *\n",
    "import math\n",
    "import keras.callbacks as callbacks\n",
    "from keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "## Loading OS libraries to configure server preferences\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import setproctitle\n",
    "SERVER_NAME = 'ultrafast'\n",
    "EXPERIMENT_TYPE='test_guided'\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "## Adding PROCESS_UC1 utilities\n",
    "sys.path.append('../../lib/TASK_2_UC1/')\n",
    "from models import *\n",
    "from util import otsu_thresholding\n",
    "from extract_xml import *\n",
    "from functions import *                   \n",
    "sys.path.append('../../lib/')\n",
    "from mlta import *\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "EXPERIMENT_TYPE=\"TEST_RANDOM\"#sys.argv[2]\n",
    "CONCEPT=\"random\"\n",
    "#c = CONCEPT.split(',')\n",
    "c_list=[\"random\"]\n",
    "#for c_ in c:\n",
    "#    c_list.append(c_.strip('[').strip(']'))\n",
    "print 'concept', CONCEPT, c_list\n",
    "CONCEPT=c_list\n",
    "\n",
    "\n",
    "cam16 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/cam16_500/patches.h5py',  'r', libver='latest', swmr=True)\n",
    "all500 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/all500/patches.h5py',  'r', libver='latest', swmr=True)\n",
    "extra17 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/extra17/patches.h5py',  'r', libver='latest', swmr=True)\n",
    "tumor_extra17=hd.File('/home/mara/adversarialMICCAI/data/ultrafast/1129-1155/patches.h5py', 'r', libver='latest', swmr=True)\n",
    "test2 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/test_data2/patches.h5py', 'r', libver='latest', swmr=True)\n",
    "pannuke= hd.File('/home/mara/adversarialMICCAI/data/ultrafast/pannuke/patches_fix.h5py', 'r', libver='latest', swmr=True)\n",
    "global data\n",
    "data={'cam16':cam16,'all500':all500,'extra17':extra17, 'tumor_extra17':tumor_extra17, 'test_data2': test2, 'pannuke':pannuke}\n",
    "datasets=data\n",
    "global concept_db\n",
    "concept_db = hd.File('../../data/normalized_cmeasures/concept_values_def.h5py','r', libver='latest', swmr=True)\n",
    "\n",
    "CONFIG_FILE = '../../doc/config.cfg'\n",
    "COLOR = True\n",
    "global new_folder\n",
    "new_folder = getFolderName()\n",
    "folder_name=EXPERIMENT_TYPE\n",
    "new_folder = '../../results/'+folder_name #new_folder\n",
    "#global error_log\n",
    "#error_log=open(new_folder+'/ERR.log', 'w')\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# SAVE FOLD\n",
    "#f=open(new_folder+\"/seed.txt\",\"w\")\n",
    "seed=1#int(sys.argv[1])\n",
    "if verbose:  print(seed)\n",
    "#f.write(str(seed))\n",
    "#f.close()\n",
    "#f = open('{}/val_by_epoch.txt'.format(new_folder), 'w')\n",
    "#ft = open('{}/train_by_epoch.txt'.format(new_folder), 'w')\n",
    "#f.close()\n",
    "#ft.close()\n",
    "    \n",
    "# SET PROCESS TITLE\n",
    "setproctitle.setproctitle('{}'.format(EXPERIMENT_TYPE))\n",
    "\n",
    "# SET SEED\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# DATA SPLIT CSVs \n",
    "train_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/train_shuffle.csv', 'r') # How is the encoding of .csv files ?\n",
    "val_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/val_shuffle.csv', 'r')\n",
    "test_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/test_shuffle.csv', 'r')\n",
    "train_list=train_csv.readlines()\n",
    "val_list=val_csv.readlines()\n",
    "test_list=test_csv.readlines()\n",
    "test2_csv = open('/mnt/nas2/results/IntermediateResults/Camelyon/test2_shuffle.csv', 'r')\n",
    "test2_list=test2_csv.readlines()\n",
    "test2_csv.close()\n",
    "train_csv.close()\n",
    "val_csv.close()\n",
    "test_csv.close()\n",
    "data_csv=open('../../doc/data_shuffle.csv', 'r')\n",
    "data_list=data_csv.readlines()\n",
    "data_csv.close()\n",
    "flog=open(\"{}/{}_log.txt\".format(new_folder, EXPERIMENT_TYPE), 'w')\n",
    "flog.write('/mnt/nas2/results/IntermediateResults/Camelyon/train_shuffle.csv')\n",
    "flog.write('/mnt/nas2/results/IntermediateResults/Camelyon/val_shuffle.csv')\n",
    "flog.write('/mnt/nas2/results/IntermediateResults/Camelyon/test_shuffle.csv')\n",
    "flog.write('/mnt/nas2/results/IntermediateResults/Camelyon/test2_shuffle.csv')\n",
    "flog.write('./doc/pannuke_data_shuffle.csv')\n",
    "flog.close()\n",
    "\n",
    "# STAIN NORMALIZATION\n",
    "def get_normalizer(patch, save_folder=''):\n",
    "    normalizer = ReinhardNormalizer()\n",
    "    normalizer.fit(patch)\n",
    "    np.save('{}/normalizer'.format(save_folder),normalizer)\n",
    "    np.save('{}/normalizing_patch'.format(save_folder), patch)\n",
    "    #print('Normalisers saved to disk.')\n",
    "    return normalizer\n",
    "def normalize_patch(patch, normalizer):\n",
    "    return np.float64(normalizer.transform(np.uint8(patch)))\n",
    "\n",
    "# LOAD DATA NORMALIZER\n",
    "global normalizer\n",
    "db_name, entry_path, patch_no = get_keys(data_list[0])\n",
    "normalization_reference_patch = data[db_name][entry_path][patch_no]\n",
    "normalizer = get_normalizer(normalization_reference_patch, save_folder=new_folder)\n",
    "\n",
    "\"\"\"\n",
    "Batch generators: \n",
    "They load a patch list: a list of file names and paths. \n",
    "They use the list to create a batch of 32 samples. \n",
    "\"\"\"\n",
    "\n",
    "# Retrieve Concept Measures\n",
    "def get_concept_measure(db_name, entry_path, patch_no, measure_type=''):\n",
    "    if measure_type=='domain':\n",
    "        return get_domain(db_name, entry_path)\n",
    "    if measure_type=='random':\n",
    "        return np.random.standard_normal()\n",
    "    path=db_name+'/'+entry_path+'/'+str(patch_no)+'/'+measure_type.strip(' ')\n",
    "    try:\n",
    "        cm=concept_db[path][0]\n",
    "        return cm\n",
    "    except:\n",
    "        print(\"[ERR]: {}, {}, {}, {} with path {}\".format(db_name, entry_path, patch_no, measure_type, path))\n",
    "        return 1.\n",
    "    \n",
    "# BATCH GENERATORS\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, patch_list, concept=CONCEPT, batch_size=32, shuffle=True, data_type=0):\n",
    "        self.batch_size=batch_size\n",
    "        self.patch_list=patch_list\n",
    "        self.shuffle=shuffle\n",
    "        self.concept = concept\n",
    "        self.data_type=data_type\n",
    "        #print 'data type:', data_type\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.patch_list)/self.batch_size))\n",
    "    def __getitem__(self, index):\n",
    "        indexes=self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        patch_list_temp=[self.patch_list[k] for k in indexes]\n",
    "        self.patch_list_temp=patch_list_temp\n",
    "        return self.__data_generation(self), None\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.patch_list))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def __data_generation(self, patch_list_temp):\n",
    "        patch_list_temp=self.patch_list_temp\n",
    "        batch_x=np.zeros((len(patch_list_temp), 224,224,3))\n",
    "        batch_y=np.zeros(len(patch_list_temp))\n",
    "        i=0\n",
    "        for line in patch_list_temp:\n",
    "            db_name, entry_path, patch_no = get_keys(line)\n",
    "            patch=data[db_name][entry_path][patch_no]\n",
    "            patch=normalize_patch(patch, normalizer)\n",
    "            patch=keras.applications.inception_v3.preprocess_input(patch) \n",
    "            label = get_class(line, entry_path) \n",
    "            if self.data_type!=0:\n",
    "                label=get_test_label(entry_path)\n",
    "            batch_x[i]=patch\n",
    "            batch_y[i]=label\n",
    "            i+=1\n",
    "        generator_output=[batch_x, batch_y]\n",
    "        for c in self.concept:\n",
    "            batch_concept_values=np.zeros(len(patch_list_temp))\n",
    "            i=0\n",
    "            for line in patch_list_temp:\n",
    "                db_name, entry_path, patch_no = get_keys(line)\n",
    "                batch_concept_values[i]=get_concept_measure(db_name, entry_path, patch_no, measure_type=c)\n",
    "                i+=1\n",
    "            if c=='domain':\n",
    "                    batch_concept_values=keras.utils.to_categorical(batch_concept_values, num_classes=7)\n",
    "            generator_output.append(batch_concept_values)\n",
    "        return generator_output\n",
    "def get_test_batch(patch_list, batch_size=32):\n",
    "    num_samples=len(patch_list)\n",
    "    while True:     \n",
    "        for offset in range(0,num_samples, batch_size):\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            #batch_concept_values={}\n",
    "            #concepts=['full_contrast','full_correlation','narea', 'ncount','nuclei_contrast','nuclei_correlation']\n",
    "            #for c in concepts:\n",
    "            #    batch_concept_values[c]=[]\n",
    "            batch_samples=patch_list[offset:offset+batch_size]\n",
    "            for line in batch_samples:\n",
    "                db_name, entry_path, patch_no = get_keys(line)\n",
    "                patch=datasets[db_name][entry_path][patch_no]\n",
    "                patch=normalize_patch(patch, normalizer)\n",
    "                patch=keras.applications.inception_v3.preprocess_input(patch)\n",
    "                #patch=keras.applications.resnet50.preprocess_input(patch)\n",
    "                label = get_test_label(entry_path)\n",
    "                batch_x.append(patch)\n",
    "                batch_y.append(label)\n",
    "            batch_x=np.asarray(batch_x, dtype=np.float32)\n",
    "            batch_y=np.asarray(batch_y, dtype=np.float32)\n",
    "            generator_output=[batch_x, batch_y]\n",
    "            for c in CONCEPT:\n",
    "                batch_concept_values=[]\n",
    "                for line in batch_samples[:(num_samples//batch_size)*batch_size]:\n",
    "                    batch_concept_values.append(get_concept_measure(db_name, entry_path, patch_no, measure_type=c))\n",
    "                batch_concept_values=np.asarray(batch_concept_values, dtype=np.float32)\n",
    "                generator_output.append(batch_concept_values)\n",
    "            yield generator_output, None\n",
    "\"\"\"\n",
    "Credits for the Gradient Reversal Layer\n",
    "https://github.com/michetonu/gradient_reversal_keras_tf/blob/master/flipGradientTF.py\n",
    "\"\"\"\n",
    "def reverse_gradient(X, hp_lambda):\n",
    "    '''Flips the sign of the incoming gradient during training.'''\n",
    "    hp_lambda = hp_lambda\n",
    "    try:\n",
    "        reverse_gradient.num_calls += 1\n",
    "    except AttributeError:\n",
    "        reverse_gradient.num_calls = 1\n",
    "    grad_name = \"GradientReversal%d\" % reverse_gradient.num_calls\n",
    "    @tf.RegisterGradient(grad_name)\n",
    "    def _flip_gradients(op, grad):\n",
    "        grad = tf.negative(grad)\n",
    "        #grad = tf.Print(grad, [grad], 'grad')\n",
    "        final_val = grad * hp_lambda \n",
    "        #final_val =tf.Print(final_val, [final_val], 'final_val')\n",
    "        return [final_val]\n",
    "    g = keras.backend.get_session().graph\n",
    "    with g.gradient_override_map({'Identity': grad_name}):\n",
    "        y = tf.identity(X)\n",
    "    return y\n",
    "\n",
    "class Hp_lambda():\n",
    "    def __init__(self,in_val):\n",
    "        self.value=in_val\n",
    "    def update(self,new_val):\n",
    "        self.value=new_val\n",
    "    def get_hyperparameter_lambda(self):\n",
    "        #val = tf.Print(self.value,[self.value],'hplambda: ')\n",
    "        return tf.Variable(self.value, name='hp_lambda')\n",
    "    #return lmb\n",
    "class GradientReversal(Layer):\n",
    "    '''Flip the sign of gradient during training.'''\n",
    "    def __init__(self, hp_lambda, **kwargs):\n",
    "        super(GradientReversal, self).__init__(**kwargs)\n",
    "        self.supports_masking = False\n",
    "        self.hp_lambda = Hp_lambda(hp_lambda)\n",
    "        #self.hp_lambda = tf.Variable(hp_lambda, name='hp_lambda')\n",
    "        #self.hp_lambda = tf.Variable(hp_lambda, name='hp_lambda')\n",
    "    def build(self, input_shape):\n",
    "        self.trainable_weights = []\n",
    "        return\n",
    "    def call(self, x, mask=None):\n",
    "        #tf.Print(self.hp_lambda, [self.hp_lambda],'self.hp_lambda: ')\n",
    "        #with tf.Session() as sess:  print(self.hp_lambda.eval()) \n",
    "        lmb=self.hp_lambda.get_hyperparameter_lambda()\n",
    "        return reverse_gradient(x, lmb)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "    def get_config(self):\n",
    "        config = {\"name\": self.__class__.__name__,\n",
    "                  'hp_lambda': keras.backend.get_value(self.hp_lambda)}\n",
    "        base_config = super(GradientReversal, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\"\"\"         \n",
    "Building guidable model \n",
    "\"\"\"\n",
    "def get_baseline_model(hp_lambda=0., c_list=[]):\n",
    "    base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "    layers_list=['conv2d_92', 'conv2d_93', 'conv2d_88', 'conv2d_89', 'conv2d_86']\n",
    "    #layers_list=[]\n",
    "    for i in range(len(base_model.layers[:])):\n",
    "        layer=base_model.layers[i]\n",
    "        if layer.name in layers_list:\n",
    "            print layer.name\n",
    "            layer.trainable=True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "    feature_output=base_model.layers[-1].output\n",
    "    gap_layer_output = keras.layers.GlobalAveragePooling2D()(feature_output)\n",
    "    feature_output = Dense(2048, activation='relu', name='finetuned_features1',kernel_regularizer=keras.regularizers.l2(0.01))(gap_layer_output) \n",
    "    feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "    feature_output = Dense(512, activation='relu', name='finetuned_features2',kernel_regularizer=keras.regularizers.l2(0.01))(feature_output)\n",
    "    feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "    feature_output = Dense(256, activation='relu', name='finetuned_features3',kernel_regularizer=keras.regularizers.l2(0.01))(feature_output)\n",
    "    feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "    #grl_layer=GradientReversal(hp_lambda=hp_lambda)\n",
    "    #feature_output_grl = grl_layer(feature_output)\n",
    "    #domain_adversarial = keras.layers.Dense(7, activation = keras.layers.Activation('softmax'), name='domain_adversarial')(feature_output_grl)\n",
    "    finetuning = Dense(1,name='predictions')(feature_output)\n",
    "    ## here you need to check how many other concepts you have apart from domain adversarial\n",
    "    # then you add one layer per each. \n",
    "    output_nodes=[finetuning]#, domain_adversarial]\n",
    "    for c in c_list:\n",
    "        if c!='domain':\n",
    "            concept_layer=  keras.layers.Dense(1, activation = keras.layers.Activation('linear'), name='extra_{}'.format(c.strip(' ')))(feature_output)\n",
    "            output_nodes.append(concept_layer)\n",
    "    model = Model(input=base_model.input, output=output_nodes)\n",
    "    #model.grl_layer=grl_layer\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "CALLBACKS\n",
    "\"\"\"\n",
    "def compute_mse(labels, predictions):\n",
    "    errors = labels - predictions\n",
    "    sum_squared_errors = np.sum(np.asarray([pow(errors[i],2) for i in range(len(errors))]))\n",
    "    mse = sum_squared_errors / len(labels)\n",
    "    return mse\n",
    "\n",
    "def evaluate(pred_, save_file=None):\n",
    "    y_true = pred_[:,0]\n",
    "    #domain_true = pred_[:,1:8]\n",
    "    true_extra_concepts={}\n",
    "    if len(c_list)>0:\n",
    "        for i in range(1, len(c_list)):\n",
    "            true_extra_concepts[i]=pred_[:,8+i]\n",
    "    #print(i)\n",
    "    y_pred = pred_[:,8+i]\n",
    "    val_acc = my_accuracy_np(y_true, y_pred)\n",
    "    #domain_pred = pred_[:, 8+i+1:8+i+1+7]\n",
    "    last_index=8+i#+7\n",
    "    pred_extra_concepts={}\n",
    "    if len(c_list)>0:\n",
    "        for i in range(1, len(c_list)):\n",
    "            pred_extra_concepts[i]=pred_[:,last_index+i]\n",
    "    #val_acc_d = accuracy_domain(domain_true, domain_pred)\n",
    "    val_r2={}\n",
    "    val_mse={}\n",
    "    if len(c_list)>0:\n",
    "        for i in range(1, len(c_list)):\n",
    "            val_r2[i] = r_square_np(true_extra_concepts[i], pred_extra_concepts[i])\n",
    "            val_mse[i] = compute_mse(true_extra_concepts[i], pred_extra_concepts[i])\n",
    "    \n",
    "    extra_string=''\n",
    "    if len(c_list)>0:\n",
    "        for i in range(1, len(c_list)):\n",
    "            extra_string=extra_string+\" {}: r2 {}, mse {}; \".format(i, val_r2[i], val_mse[i])\n",
    "    #print(\"Acc: {}, Acc domain: {}\\n\".format(val_acc, val_acc_d)+extra_string)\n",
    "    #save_file.write(\"Val acc: {}, acc_domain: {}\\n\".format(val_acc, val_acc_d)+extra_string)\n",
    "    \n",
    "\n",
    "class eval_model(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        f = open('{}/val_by_epoch.txt'.format(new_folder), 'a')\n",
    "        ft = open('{}/train_by_epoch.txt'.format(new_folder), 'a')\n",
    "        pred_ = self.model.predict_generator(val_generator2, steps = len(val_list)// BATCH_SIZE, workers=4, use_multiprocessing=False)#// hvd.size()) \n",
    "        #import pdb; pdb.set_trace()\n",
    "        y_true = pred_[:,0]\n",
    "        y_pred = pred_[:,2]\n",
    "        #print 'y_true: ', y_true\n",
    "        #print 'y_pred: ', y_pred\n",
    "        val_acc = my_accuracy_np(y_true, y_pred)\n",
    "        cm_true = pred_[:, 1]\n",
    "        cm_pred = pred_[:,3]\n",
    "        #print 'cm_true: ', cm_true\n",
    "        #print 'cm_pred: ', cm_pred\n",
    "        \n",
    "        val_r2 = r_square_np(cm_true, cm_pred)\n",
    "        val_mse = compute_mse(cm_true, cm_pred)\n",
    "        print(\"Val acc: {}, r2: {}, MSE: {}\\n\".format(val_acc, val_r2, val_mse))\n",
    "        report_val_acc.append(val_acc)\n",
    "        report_val_r2.append(val_r2)\n",
    "        report_val_mse.append(val_mse)\n",
    "        f.write(\"Val acc: {}, r2: {}, mse: {}\\n\".format(val_acc, val_r2, val_mse))\n",
    "        \n",
    "        train_pred_ = self.model.predict_generator(train_generator2, steps=100, workers=1, use_multiprocessing=False)\n",
    "        cm_true = train_pred_[:, 1]\n",
    "        cm_pred = train_pred_[:,3]\n",
    "        #print 'train_cmtrue: ', cm_true\n",
    "        #print 'train_cmpred: ', cm_pred\n",
    "        train_r2 = r_square_np(cm_true, cm_pred)\n",
    "        train_mse = compute_mse(cm_true, cm_pred)\n",
    "        print(\"Train r2: {}, MSE: {}\".format(train_r2, train_mse))\n",
    "        ft.write(\"Train acc: {}, r2: {}, mse: {}\\n\".format(val_acc, val_r2, val_mse))\n",
    "      \n",
    "logdir='{}/tb_log'.format(new_folder)\n",
    "checkpoint_dir = '{}'.format(new_folder) \n",
    "callbacks = [LR_scheduling(new_folder=new_folder, loss=None, metrics=None), \n",
    "             eval_model(),\n",
    "             keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, \n",
    "                                           patience=10),\n",
    "            keras.callbacks.ModelCheckpoint('{}/best_model.h5'.format(checkpoint_dir), monitor='val_loss', mode='min', save_best_only=True, save_weights_only=True, verbose=1)\n",
    "            ]\n",
    "# END Callbacks\n",
    "\"\"\" \n",
    "LOSS FUNCTIONS\n",
    "\"\"\"\n",
    "def keras_mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.losses.mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def bbce(y_true, y_pred):\n",
    "    # we use zero weights to set the loss to zero for unlabeled data\n",
    "    verbose=0\n",
    "    zero= tf.constant(-1, dtype=tf.float32)\n",
    "    where = tf.not_equal(y_true, zero)\n",
    "    where = tf.reshape(where, [-1])\n",
    "    indices=tf.where(where) #indices where the item of y_true is NOT -1\n",
    "    indices = tf.reshape(indices, [-1])\n",
    "    sliced_y_true = tf.nn.embedding_lookup(y_true, indices)\n",
    "    sliced_y_pred = tf.nn.embedding_lookup(y_pred, indices)\n",
    "    n1 = tf.shape(indices)[0] #number of train images in batch\n",
    "    batch_size = tf.shape(y_true)[0]\n",
    "    n2 = batch_size - n1 #number of test images in batch\n",
    "    sliced_y_true = tf.reshape(sliced_y_true, [n1, -1])\n",
    "    n1_ = tf.cast(n1, tf.float32)\n",
    "    n2_ = tf.cast(n2, tf.float32)\n",
    "    multiplier = (n1_+ n2_) / n1_\n",
    "    zero_class = tf.constant(0, dtype=tf.float32)\n",
    "    where_class_is_zero=tf.cast(tf.reduce_sum(tf.cast(tf.equal(sliced_y_true, zero_class), dtype=tf.float32)), dtype=tf.float32)\n",
    "    if verbose:\n",
    "        where_class_is_zero=tf.Print(where_class_is_zero,[where_class_is_zero],'where_class_is_zero: ')\n",
    "    class_weight_zero = tf.cast(tf.divide(n1_, 2. * tf.cast(where_class_is_zero, dtype=tf.float32)+0.001), dtype=tf.float32)\n",
    "    \n",
    "    if verbose:\n",
    "        class_weight_zero=tf.Print(class_weight_zero,[class_weight_zero],'class_weight_zero: ')\n",
    "    one_class = tf.constant(1, dtype=tf.float32)\n",
    "    where_class_is_one=tf.cast(tf.reduce_sum(tf.cast(tf.equal(sliced_y_true, one_class), dtype=tf.float32)), dtype=tf.float32)\n",
    "    if verbose:\n",
    "        where_class_is_one=tf.Print(where_class_is_one,[where_class_is_one],'where_class_is_one: ')\n",
    "        n1_=tf.Print(n1_,[n1_],'n1_: ')\n",
    "    class_weight_one = tf.cast(tf.divide(n1_, 2. * tf.cast(where_class_is_one,dtype=tf.float32)+0.001), dtype=tf.float32)\n",
    "    class_weight_zero =  tf.constant(23477.0/(23477.0+123820.0), dtype=tf.float32)\n",
    "    class_weight_one =  tf.constant(123820.0/(23477.0+123820.0), dtype=tf.float32)\n",
    "    A = tf.ones(tf.shape(sliced_y_true), dtype=tf.float32) - sliced_y_true \n",
    "    A = tf.scalar_mul(class_weight_zero, A)\n",
    "    B = tf.scalar_mul(class_weight_one, sliced_y_true) \n",
    "    class_weight_vector=A+B\n",
    "    ce = tf.nn.sigmoid_cross_entropy_with_logits(labels=sliced_y_true,logits=sliced_y_pred)\n",
    "    ce = tf.multiply(class_weight_vector,ce)\n",
    "    return tf.reduce_mean(ce)\n",
    "\n",
    "from keras.initializers import Constant\n",
    "global domain_weight\n",
    "global main_task_weight\n",
    "\n",
    "class CustomMultiLossLayer(Layer):\n",
    "    def __init__(self, new_folder='', nb_outputs=2, **kwargs):\n",
    "        self.nb_outputs = nb_outputs\n",
    "        self.is_placeholder = True\n",
    "        super(CustomMultiLossLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape=None):\n",
    "        # initialise log_vars\n",
    "        self.log_vars = []\n",
    "        for i in range(self.nb_outputs):\n",
    "            self.log_vars += [self.add_weight(name='log_var' + str(i), shape=(1,),\n",
    "                                              initializer=Constant(0.), trainable=True)]\n",
    "        super(CustomMultiLossLayer, self).build(input_shape)\n",
    "    \"\"\"\n",
    "    def multi_loss(self, ys_true, ys_pred):\n",
    "        assert len(ys_true) == self.nb_outputs and len(ys_pred) == self.nb_outputs\n",
    "        loss = 0\n",
    "        for y_true, y_pred, log_var in zip(ys_true, ys_pred, self.log_vars):\n",
    "            precision = K.exp(-log_var[0])\n",
    "            loss += K.sum(precision * (y_true - y_pred)**2. + log_var[0], -1)\n",
    "        return K.mean(loss)\n",
    "    \"\"\"\n",
    "    def multi_loss(self,  ys_true, ys_pred):\n",
    "        assert len(ys_true) == self.nb_outputs and len(ys_pred) == self.nb_outputs\n",
    "        loss = 0\n",
    "        i=0\n",
    "        for y_true, y_pred, log_var in zip(ys_true, ys_pred, self.log_vars):\n",
    "            precision =keras.backend.exp(-log_var[0]) \n",
    "            if i==0:\n",
    "                pred_loss = bbce(y_true, y_pred)\n",
    "                term = main_task_weight*precision*pred_loss + main_task_weight*0.5 * log_var[0]  \n",
    "                #term=tf.Print(keras.backend.mean(term), [keras.backend.mean(term)], 'mean bbce: ')\n",
    "            #elif i==1:\n",
    "                # I need to find a better way for this\n",
    "            #    pred_loss = keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "                #keras_mse(y_true, y_pred)\n",
    "            #    term =  domain_weight * precision * pred_loss + domain_weight * log_var[0]\n",
    "                #term=tf.Print(keras.backend.mean(term), [keras.backend.mean(term)], 'mean cce: ')\n",
    "            else:\n",
    "                pred_loss = keras_mse(y_true, y_pred)\n",
    "                #pred_loss=tf.Print(pred_loss, [pred_loss], 'MSE: ')\n",
    "                term = 0.5 * precision * pred_loss + 0.5 * log_var[0]\n",
    "            loss+=term\n",
    "            term = 0.\n",
    "            i+=1\n",
    "        return keras.backend.mean(loss)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        ys_true = inputs[:self.nb_outputs]\n",
    "        ys_pred = inputs[self.nb_outputs:]\n",
    "        loss = self.multi_loss(ys_true, ys_pred)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return keras.backend.concatenate(inputs, -1)\n",
    "\n",
    "\"\"\"\n",
    "EVALUATION FUNCTIONs\n",
    "\"\"\"\n",
    "def accuracy_domain(y_true,y_pred):\n",
    "    #y_p_r=\n",
    "    y_p_r = np.asarray([np.argmax(y_pred[i,:]) for i in range(len(y_pred[:,0]))])\n",
    "    #y_p_r=np.round(y_pred)\n",
    "    y_true = np.asarray([np.argmax(y_true[i,:]) for i in range(len(y_true[:,0]))])\n",
    "    acc = np.equal(y_p_r, y_true)**1.\n",
    "    acc = np.mean(np.float32(acc))\n",
    "    return acc\n",
    "def my_sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def my_accuracy_np(y_true, y_pred):\n",
    "    sliced_y_pred = my_sigmoid(y_pred)\n",
    "    y_pred_rounded = np.round(sliced_y_pred)\n",
    "    acc = np.equal(y_pred_rounded, y_true)**1.\n",
    "    acc = np.mean(np.float32(acc))\n",
    "    return acc\n",
    "def r_square_np(y_true, y_pred):\n",
    "    SS_res =  np.sum(np.square(y_true - y_pred))\n",
    "    SS_tot = np.sum(np.square(y_true - np.mean(y_true)))\n",
    "    r2_mine=( 1 - SS_res/(SS_tot + keras.backend.epsilon()) )\n",
    "    return ( 1 - SS_res/(SS_tot + keras.backend.epsilon()) )\n",
    "\n",
    "global report_val_acc \n",
    "global report_val_r2\n",
    "global report_val_mse\n",
    "report_val_acc=[]\n",
    "report_val_r2=[]\n",
    "report_val_mse=[]\n",
    "# LOG FILE\n",
    "global log_file\n",
    "\n",
    "\"\"\"\n",
    "DATA GENERATORS CREATION\n",
    "\"\"\"\n",
    "train_generator=DataGenerator(data_list, concept=CONCEPT, batch_size=BATCH_SIZE, data_type=0)\n",
    "train_generator2=DataGenerator(data_list, concept=CONCEPT, batch_size=BATCH_SIZE, data_type=1)#get_batch_data(data_list, batch_size=BATCH_SIZE)\n",
    "val_generator=DataGenerator(val_list, concept=CONCEPT, batch_size=BATCH_SIZE, data_type=1) #get_test_batch(val_list, batch_size=BATCH_SIZE)\n",
    "val_generator2= DataGenerator(val_list, concept=CONCEPT, batch_size=BATCH_SIZE, data_type=1) #get_test_batch(val_list, batch_size=BATCH_SIZE)\n",
    "test_generator= DataGenerator(test_list, concept=CONCEPT, batch_size=BATCH_SIZE, data_type=1) #get_test_batch(test_list, batch_size=BATCH_SIZE)\n",
    "\n",
    "\"\"\" \n",
    "Get trainable model with Hepistemic Uncertainty Weighted Loss \n",
    "\"\"\"\n",
    "\n",
    "def get_trainable_model(baseline_model):\n",
    "    inp = keras.layers.Input(shape=(224,224,3,), name='inp')\n",
    "    y1_pred, y2_pred = baseline_model(inp)\n",
    "    y1_true=keras.layers.Input(shape=(1,),name='y1_true')\n",
    "    #y2_true=keras.layers.Input(shape=(7,),name='y2_true')\n",
    "    out = CustomMultiLossLayer(nb_outputs=2, new_folder=new_folder)([y1_true, y2_true, y1_pred, y2_pred])\n",
    "    return Model(input=[inp, y1_true, y2_true], output=out)\n",
    "\n",
    "def get_trainable_model(baseline_model):\n",
    "    inp = keras.layers.Input(shape=(224,224,3,), name='inp')\n",
    "    outputs = baseline_model(inp)\n",
    "    n_extra_concepts = len(outputs) -1\n",
    "    print(n_extra_concepts)\n",
    "    y_true=keras.layers.Input(shape=(1,),name='y_true')\n",
    "    #domain_true=keras.layers.Input(shape=(7,),name='domain_true')\n",
    "    extra_concepts_true=[]\n",
    "    for i in range(n_extra_concepts):\n",
    "        print('extra_{}'.format(i))\n",
    "        extra_true=keras.layers.Input(shape=(1,), name='extra_{}'.format(i))\n",
    "        extra_concepts_true.append(extra_true)\n",
    "    new_model_input=[inp, y_true]#, domain_true]\n",
    "    loss_inputs=[y_true]#, domain_true]\n",
    "    for i in range(len(extra_concepts_true)):\n",
    "        new_model_input.append(extra_concepts_true[i])\n",
    "        loss_inputs.append(extra_concepts_true[i])\n",
    "    for out_ in outputs:\n",
    "        loss_inputs.append(out_)\n",
    "    out = CustomMultiLossLayer(nb_outputs=len(outputs), new_folder=new_folder)(loss_inputs)\n",
    "    return Model(input=new_model_input, output=out)\n",
    "\n",
    "verbose=True\n",
    "def custom_train_model(hu_model,\n",
    "                       epochs= 1, \n",
    "                       lr=1e-4, verbose=True,\n",
    "                       train_generator=train_generator,\n",
    "                       val_generator=val_generator,\n",
    "                       grl_layer=None,\n",
    "                      ):\n",
    "    opt = keras.optimizers.SGD(lr=lr, momentum=0.9, nesterov=True)\n",
    "    compile_model(hu_model,opt,loss=None,metrics=None)\n",
    "    history = hu_model.fit_generator(train_generator,\n",
    "                    steps_per_epoch= len(data_list) // (BATCH_SIZE ),\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=epochs,\n",
    "                    verbose=verbose,\n",
    "                    workers=4,\n",
    "                    use_multiprocessing=False,\n",
    "                    validation_data= val_generator,\n",
    "                    validation_steps= len(val_list)//BATCH_SIZE) \n",
    "    return hu_model\n",
    "\n",
    "\"\"\"\n",
    "Training \n",
    "\"\"\"\n",
    "def evaluate_model(d_list, model, batch_size=BATCH_SIZE, test_type=''):\n",
    "    batch_size=32\n",
    "    #d_list=test_list[:100]\n",
    "    #t_gen=DataGenerator(d_list, concept=CONCEPT, batch_size=BATCH_SIZE, data_type=0)\n",
    "    test_generator_=get_test_batch(d_list, batch_size=batch_size)\n",
    "    steps=len(d_list)//batch_size\n",
    "    print steps\n",
    "    initial_lr = 1e-4\n",
    "    opt = keras.optimizers.SGD(lr=initial_lr, momentum=0.9, nesterov=True)\n",
    "    compile_model(t_m,opt,loss=None,metrics=None)\n",
    "    callbacks = []\n",
    "    y_true=np.zeros(len(d_list))\n",
    "    y_pred=np.zeros((len(d_list),1))\n",
    "    N=0\n",
    "    all_cm=np.zeros(len(d_list))\n",
    "    all_p_cm=np.zeros(len(d_list))\n",
    "\n",
    "    while N<len(d_list):\n",
    "        #x_b, y_b, cm_b = t_gen.__getitem__(N)\n",
    "        [x_b, y_b, cm_b], _ = test_generator_.next()\n",
    "        pred_ = t_m.predict([x_b, y_b, cm_b])\n",
    "        y_p_b = pred_[:,2]\n",
    "        cm_p_b = pred_[:,3]\n",
    "\n",
    "        y_true[N:N+len(y_b)]=y_b.reshape(len(y_b))\n",
    "        y_pred[N:N+len(y_p_b)]=y_p_b.reshape(len(y_p_b),1)\n",
    "        all_p_cm[N:N+len(cm_p_b)]=cm_p_b.reshape(len(cm_p_b))\n",
    "        all_cm[N:N+len(cm_b)]=cm_b\n",
    "        N+=len(y_p_b)\n",
    "    y_true=y_true.reshape((len(d_list),1))\n",
    "    acc = my_accuracy(y_true, y_pred).eval(session=tf.Session())\n",
    "    sliced_y_pred = tf.sigmoid(y_pred)\n",
    "    y_pred_rounded = K.round(sliced_y_pred)\n",
    "    acc_sc = accuracy_score(y_pred_rounded.eval(session=tf.Session()), y_true)\n",
    "    print('accuracy: ', acc_sc)\n",
    "    \n",
    "    y_pred = sliced_y_pred.eval(session=tf.Session())\n",
    "    #sliced_y_pred = tf.sigmoid(y_pred)\n",
    "    #y_pred_rounded = K.round(sliced_y_pred)\n",
    "    auc_score=sklearn.metrics.roc_auc_score(y_true,sliced_y_pred.eval(session=tf.Session()))\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(1):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_pred.ravel())\n",
    "    \"\"\"\n",
    "    roc_auc[\"micro\"] = auc_score\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example AUC = {}'.format(roc_auc[0]))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show() \"\"\"\n",
    "    print 'auc: {}'.format(auc_score)\n",
    "    print roc_auc[0]\n",
    "    #auc_record = open('{}/auc_{}.txt'.format(model_folder,test_type), 'w')\n",
    "    #auc_record.write('{}'.format(roc_auc[0]))\n",
    "    #auc_record.close()\n",
    "    return all_cm, all_p_cm, auc_score\n",
    "starting_time = time.time()\n",
    "print(\"training\")\n",
    "print(\"phase 1\")\n",
    "main_task_weight=1. \n",
    "domain_weight = 1.\n",
    "model = get_baseline_model(hp_lambda=0., c_list=c_list)\n",
    "t_m = get_trainable_model(model)\n",
    "t_m_ = get_trainable_model(model)\n",
    "\"\"\"\n",
    "t_m = custom_train_model(t_m, epochs=80,\n",
    "                         lr=1e-4, grl_layer=None)\n",
    "base_model_weights=model.get_weights()\n",
    "print(\"phase 2\")\n",
    "main_task_weight=1.\n",
    "domain_weight=1. \n",
    "model=get_baseline_model(hp_lambda=1., c_list=c_list)\n",
    "model.set_weights(base_model_weights)\n",
    "\n",
    "t_m = custom_train_model(t_m_, epochs=20, \n",
    "                         lr=1e-4, grl_layer=None)\"\"\"\n",
    "\"\"\"\n",
    "# 20 epochs: gradient reversal\n",
    "# not run yet\n",
    "#print(\"phase 3\")\n",
    "#main_task_weight=1. \n",
    "#domain_weight = 1e-2\n",
    "#model = get_baseline_model(hp_lambda=-1.)\n",
    "#t_m = get_trainable_model(model)\n",
    "#t_m = custom_train_model(t_m, epochs=5s, lr=1e-4, grl_layer=model.grl_layer)\n",
    "#base_model_weights=model.get_weights()\n",
    "\n",
    "main_task_weight=1.\n",
    "domain_weight=1. #0. #(higher learning rate is needed for this task)\n",
    "model=get_baseline_model(hp_lambda=10.)\n",
    "model.set_weights(base_model_weights)\n",
    "#model.compile(opt)\n",
    "t_m_ = get_trainable_model(model)\n",
    "t_m = custom_train_model(t_m_,epochs=5, lr=1e-4, grl_layer=model.grl_layer)\n",
    "# We run some more iterations on the main task\n",
    "main_task_weight=1. \n",
    "domain_weight = 1\n",
    "model=get_baseline_model(hp_lambda=10.)\n",
    "model.set_weights(base_model_weights)\n",
    "#model.compile(opt)\n",
    "t_m = get_trainable_model(model)\n",
    "t_m = custom_train_model(t_m, epochs=2, lr=1e-4)\n",
    "\"\"\"\n",
    "# end of scheduling \n",
    "end_time=time.time()\n",
    "starting_time=0.\n",
    "total_training_time = end_time - starting_time \n",
    "#\n",
    "\n",
    "model_folder='/home/mara/multitask_adversarial/results/RANDOM_2409/'\n",
    "#t_m = get_trainable_model(model)\n",
    "t_m.load_weights(model_folder+'/best_model.h5')\n",
    "full_list=test_list+test2_list\n",
    "all_cm_t, all_p_cm_t, roc_auc=evaluate_model(full_list,t_m, test_type='overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mara/multitask_adversarial/results/RANDOM_2409/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "('accuracy: ', 0.7649226234340457)\n",
      "auc: 0.833264975004\n",
      "0.833264975003912\n"
     ]
    }
   ],
   "source": [
    "all_cm_t, all_p_cm_t, roc_auc=evaluate_model(test_list,t_m, test_type='overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "('accuracy: ', 0.689)\n",
      "auc: 0.877276\n",
      "0.8772759999999998\n"
     ]
    }
   ],
   "source": [
    "all_cm_t, all_p_cm_t, roc_auc=evaluate_model(test2_list,t_m, test_type='overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8275, 0.004330127018922197)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.82, 0.83, 0.83, 0.83]), np.std([[0.82, 0.83, 0.83, 0.83]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8775, 0.004330127018922197)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.87, 0.88, 0.88, 0.88]), np.std([0.87, 0.88, 0.88, 0.88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.820651, 0.81333)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.820651,0.81333,0.8173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8362, 0.83309)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8362, 0.83309, 0.8333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.885, 0.8763)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.885, 0.8763, 0.8773"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
