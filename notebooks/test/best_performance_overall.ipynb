{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "res_folders=os.listdir('../../results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder='/home/mara/multitask_adversarial/results/DOMAIN-COUNT/'\n",
    "CONCEPT=['domain','ncount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../doc/data_shuffle.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'../../doc/data_shuffle.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n",
      "Using brightness standardization\n"
     ]
    }
   ],
   "source": [
    "## Loading OS libraries to configure server preferences\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import setproctitle\n",
    "SERVER_NAME = 'ultrafast'\n",
    "EXPERIMENT_TYPE='test_domain'\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "## Adding PROCESS_UC1 utilities\n",
    "sys.path.append('../../lib/TASK_2_UC1/')\n",
    "from models import *\n",
    "from util import otsu_thresholding\n",
    "from extract_xml import *\n",
    "from functions import *                   \n",
    "sys.path.append('../../lib/')\n",
    "from mlta import *\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(0)# str(hvd.local_rank())\n",
    "keras.backend.set_session(tf.Session(config=config))\n",
    "\n",
    "verbose=1 \n",
    "\"\"\"loading dataset files\"\"\"\n",
    "#rank = MPI.COMM_WORLD.rank\n",
    "cam16 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/cam16_500/patches.h5py',  'r', libver='latest', swmr=True)\n",
    "all500 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/all500/patches.h5py',  'r', libver='latest', swmr=True)\n",
    "extra17 = hd.File('/home/mara/adversarialMICCAI/data/ultrafast/extra17/patches.h5py','r', libver='latest', swmr=True)\n",
    "tumor_extra17=hd.File('/home/mara/adversarialMICCAI/data/ultrafast/1129-1155/patches.h5py', 'r', libver='latest', swmr=True)\n",
    "test2 = hd.File('/mnt/nas2/results/IntermediateResults/Camelyon/ultrafast/test_data2/patches.hdf5', 'r', libver='latest', swmr=True)\n",
    "pannuke= hd.File('/mnt/nas2/results/IntermediateResults/Camelyon/pannuke/patches_fix.hdf5', 'r', libver='latest', swmr=True)\n",
    "\n",
    "global datasetss\n",
    "datasetss={'cam16':cam16,'all500':all500,'extra17':extra17, 'tumor_extra17':tumor_extra17, 'test_data2': test2, 'pannuke':pannuke}\n",
    "\n",
    "global concept_db\n",
    "concept_db = hd.File('../../data/normalized_cmeasures/concept_values_def.h5py','r')\n",
    "\n",
    "#SYSTEM CONFIGS \n",
    "CONFIG_FILE = '../../doc/config.cfg'\n",
    "COLOR = True\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# SAVE FOLD\n",
    "f=open(model_folder+\"/seed.txt\",\"r\")\n",
    "seed=1001#int(f.read())\n",
    "if verbose:  print(seed)\n",
    "#f.write(str(seed))\n",
    "f.close()\n",
    "\n",
    "\n",
    "# SET PROCESS TITLE\n",
    "setproctitle.setproctitle('{}'.format(EXPERIMENT_TYPE))\n",
    "\n",
    "# SET SEED\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# DATA SPLIT CSVs \n",
    "train_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/train_shuffle.csv', 'r') # How is the encoding of .csv files ?\n",
    "val_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/val_shuffle.csv', 'r')\n",
    "test_csv=open('/mnt/nas2/results/IntermediateResults/Camelyon/test_shuffle.csv', 'r')\n",
    "train_list=train_csv.readlines()\n",
    "val_list=val_csv.readlines()\n",
    "test_list=test_csv.readlines()\n",
    "test2_csv = open('/mnt/nas2/results/IntermediateResults/Camelyon/test2_shuffle.csv', 'r')\n",
    "test2_list=test2_csv.readlines()\n",
    "test2_csv.close()\n",
    "train_csv.close()\n",
    "val_csv.close()\n",
    "test_csv.close()\n",
    "data_csv=open('../../doc/data_shuffle.csv', 'r')\n",
    "data_list=data_csv.readlines()\n",
    "data_csv.close()\n",
    "\n",
    "# STAIN NORMALIZATION\n",
    "def get_normalizer(patch, save_folder='../../results/'):\n",
    "    normalizer = ReinhardNormalizer()\n",
    "    normalizer.fit(patch)\n",
    "    np.save('{}/normalizer'.format(save_folder),normalizer)\n",
    "    np.save('{}/normalizing_patch'.format(save_folder), patch)\n",
    "    #print('Normalisers saved to disk.')\n",
    "    return normalizer\n",
    "def normalize_patch(patch, normalizer):\n",
    "    return np.float64(normalizer.transform(np.uint8(patch)))\n",
    "\n",
    "global normalizer\n",
    "db_name, entry_path, patch_no = get_keys(data_list[0])\n",
    "normalization_reference_patch = datasetss[db_name][entry_path][patch_no]\n",
    "normalizer = get_normalizer(normalization_reference_patch, save_folder='../../results/')\n",
    "\n",
    "\"\"\"\n",
    "Batch generators: \n",
    "They load a patch list: a list of file names and paths. \n",
    "They use the list to create a batch of 32 samples. \n",
    "\"\"\"\n",
    "\n",
    "# Retrieve Concept Measures\n",
    "def get_concept_measure(db_name, entry_path, patch_no, measure_type=''):\n",
    "    if measure_type=='domain':\n",
    "        return get_domain(db_name, entry_path)\n",
    "    path=db_name+'/'+entry_path+'/'+str(patch_no)+'/'+measure_type.strip(' ')\n",
    "    try:\n",
    "        cm=concept_db[path][0]\n",
    "        return cm\n",
    "    except:\n",
    "        print(\"[ERR]: {}, {}, {}, {} with path {}\".format(db_name, entry_path, patch_no, measure_type, path))\n",
    "        #import pdb; pdb.set_trace()\n",
    "        return 1.\n",
    "    \n",
    "# BATCH GENERATORS\n",
    "import keras.utils\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, patch_list, concept=CONCEPT, batch_size=32, shuffle=True, data_type=0):\n",
    "        self.batch_size=batch_size\n",
    "        self.patch_list=patch_list\n",
    "        self.shuffle=shuffle\n",
    "        self.concept = concept\n",
    "        self.data_type=data_type\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.patch_list)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes=self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        patch_list_temp=[self.patch_list[k] for k in indexes]\n",
    "        self.patch_list_temp=patch_list_temp\n",
    "        return self.__data_generation(self), None\n",
    "    \n",
    "    def get(self, index):\n",
    "        indexes=self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        patch_list_temp=[self.patch_list[k] for k in indexes]\n",
    "        self.patch_list_temp=patch_list_temp\n",
    "        return self.__data_generation(self), None\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.patch_list))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __data_generation(self, patch_list_temp):\n",
    "        patch_list_temp=self.patch_list_temp\n",
    "        batch_x=np.zeros((len(patch_list_temp), 224,224,3))\n",
    "        batch_y=np.zeros(len(patch_list_temp))\n",
    "        i=0\n",
    "        for line in patch_list_temp:\n",
    "            db_name, entry_path, patch_no = get_keys(line)\n",
    "            patch=datasetss[db_name][entry_path][patch_no]\n",
    "            patch=normalize_patch(patch, normalizer)\n",
    "            patch=keras.applications.inception_v3.preprocess_input(patch) \n",
    "            label = get_class(line, entry_path) \n",
    "            if self.data_type!=0:\n",
    "                label=get_test_label(entry_path)\n",
    "            batch_x[i]=patch\n",
    "            batch_y[i]=label\n",
    "            i+=1\n",
    "        generator_output=[batch_x, batch_y]\n",
    "        for c in self.concept:\n",
    "            batch_concept_values=np.zeros(len(patch_list_temp))\n",
    "            i=0\n",
    "            for line in patch_list_temp:\n",
    "                db_name, entry_path, patch_no = get_keys(line)\n",
    "                batch_concept_values[i]=get_concept_measure(db_name, entry_path, patch_no, measure_type=c)\n",
    "                i+=1\n",
    "            if c=='domain':\n",
    "                    batch_concept_values=keras.utils.to_categorical(batch_concept_values, num_classes=7)\n",
    "            generator_output.append(batch_concept_values)\n",
    "        return generator_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib as mpl   \n",
    "#mpl.use('Agg')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "from keras import *\n",
    "import setproctitle\n",
    "SERVER_NAME = 'ultrafast'\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "## Adding PROCESS_UC1 utilities\n",
    "sys.path.append('../../lib/TASK_2_UC1/')\n",
    "from models import *\n",
    "from util import otsu_thresholding\n",
    "from extract_xml import *\n",
    "from functions import *                   \n",
    "sys.path.append('../../lib/')\n",
    "from mlta import *\n",
    "import math\n",
    "import keras.callbacks as callbacks\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\"\"\" \n",
    "Get trainable model with Hepistemic Uncertainty Weighted Loss \n",
    "\"\"\"\n",
    "def get_trainable_model(baseline_model):\n",
    "    inp = keras.layers.Input(shape=(224,224,3,), name='inp')\n",
    "    outputs = baseline_model(inp)\n",
    "    n_extra_concepts = len(outputs) -2\n",
    "    print(n_extra_concepts)\n",
    "    y_true=keras.layers.Input(shape=(1,),name='y_true')\n",
    "    domain_true=keras.layers.Input(shape=(7,),name='domain_true')\n",
    "    extra_concepts_true=[]\n",
    "    for i in range(n_extra_concepts):\n",
    "        print('extra_{}'.format(i))\n",
    "        extra_true=keras.layers.Input(shape=(1,), name='extra_{}'.format(i))\n",
    "        extra_concepts_true.append(extra_true)\n",
    "    new_model_input=[inp, y_true, domain_true]\n",
    "    loss_inputs=[y_true, domain_true]\n",
    "    for i in range(len(extra_concepts_true)):\n",
    "        new_model_input.append(extra_concepts_true[i])\n",
    "        loss_inputs.append(extra_concepts_true[i])\n",
    "    for out_ in outputs:\n",
    "        loss_inputs.append(out_)\n",
    "    out = CustomMultiLossLayer(nb_outputs=len(outputs), new_folder='')(loss_inputs)\n",
    "    return Model(input=new_model_input, output=out)\n",
    "\"\"\" \n",
    "LOSS FUNCTIONS\n",
    "\"\"\"\n",
    "def keras_mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.losses.mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def bbce(y_true, y_pred):\n",
    "    # we use zero weights to set the loss to zero for unlabeled data\n",
    "    verbose=0\n",
    "    zero= tf.constant(-1, dtype=tf.float32)\n",
    "    where = tf.not_equal(y_true, zero)\n",
    "    where = tf.reshape(where, [-1])\n",
    "    indices=tf.where(where) #indices where the item of y_true is NOT -1\n",
    "    indices = tf.reshape(indices, [-1])\n",
    "    sliced_y_true = tf.nn.embedding_lookup(y_true, indices)\n",
    "    sliced_y_pred = tf.nn.embedding_lookup(y_pred, indices)\n",
    "    n1 = tf.shape(indices)[0] #number of train images in batch\n",
    "    batch_size = tf.shape(y_true)[0]\n",
    "    n2 = batch_size - n1 #number of test images in batch\n",
    "    sliced_y_true = tf.reshape(sliced_y_true, [n1, -1])\n",
    "    n1_ = tf.cast(n1, tf.float32)\n",
    "    n2_ = tf.cast(n2, tf.float32)\n",
    "    multiplier = (n1_+ n2_) / n1_\n",
    "    zero_class = tf.constant(0, dtype=tf.float32)\n",
    "    where_class_is_zero=tf.cast(tf.reduce_sum(tf.cast(tf.equal(sliced_y_true, zero_class), dtype=tf.float32)), dtype=tf.float32)\n",
    "    if verbose:\n",
    "        where_class_is_zero=tf.Print(where_class_is_zero,[where_class_is_zero],'where_class_is_zero: ')\n",
    "    class_weight_zero = tf.cast(tf.divide(n1_, 2. * tf.cast(where_class_is_zero, dtype=tf.float32)+0.001), dtype=tf.float32)\n",
    "    \n",
    "    if verbose:\n",
    "        class_weight_zero=tf.Print(class_weight_zero,[class_weight_zero],'class_weight_zero: ')\n",
    "    one_class = tf.constant(1, dtype=tf.float32)\n",
    "    where_class_is_one=tf.cast(tf.reduce_sum(tf.cast(tf.equal(sliced_y_true, one_class), dtype=tf.float32)), dtype=tf.float32)\n",
    "    if verbose:\n",
    "        where_class_is_one=tf.Print(where_class_is_one,[where_class_is_one],'where_class_is_one: ')\n",
    "        n1_=tf.Print(n1_,[n1_],'n1_: ')\n",
    "    class_weight_one = tf.cast(tf.divide(n1_, 2. * tf.cast(where_class_is_one,dtype=tf.float32)+0.001), dtype=tf.float32)\n",
    "    class_weight_zero =  tf.constant(23477.0/(23477.0+123820.0), dtype=tf.float32)\n",
    "    class_weight_one =  tf.constant(123820.0/(23477.0+123820.0), dtype=tf.float32)\n",
    "    A = tf.ones(tf.shape(sliced_y_true), dtype=tf.float32) - sliced_y_true \n",
    "    A = tf.scalar_mul(class_weight_zero, A)\n",
    "    B = tf.scalar_mul(class_weight_one, sliced_y_true) \n",
    "    class_weight_vector=A+B\n",
    "    ce = tf.nn.sigmoid_cross_entropy_with_logits(labels=sliced_y_true,logits=sliced_y_pred)\n",
    "    ce = tf.multiply(class_weight_vector,ce)\n",
    "    return tf.reduce_mean(ce)\n",
    "\n",
    "from keras.initializers import Constant\n",
    "global domain_weight\n",
    "global main_task_weight\n",
    "\n",
    "class CustomMultiLossLayer(Layer):\n",
    "    def __init__(self, new_folder='', nb_outputs=2, **kwargs):\n",
    "        self.nb_outputs = nb_outputs\n",
    "        self.is_placeholder = True\n",
    "        super(CustomMultiLossLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape=None):\n",
    "        # initialise log_vars\n",
    "        self.log_vars = []\n",
    "        for i in range(self.nb_outputs):\n",
    "            self.log_vars += [self.add_weight(name='log_var' + str(i), shape=(1,),\n",
    "                                              initializer=Constant(0.), trainable=True)]\n",
    "        super(CustomMultiLossLayer, self).build(input_shape)\n",
    "    \"\"\"\n",
    "    def multi_loss(self, ys_true, ys_pred):\n",
    "        assert len(ys_true) == self.nb_outputs and len(ys_pred) == self.nb_outputs\n",
    "        loss = 0\n",
    "        for y_true, y_pred, log_var in zip(ys_true, ys_pred, self.log_vars):\n",
    "            precision = K.exp(-log_var[0])\n",
    "            loss += K.sum(precision * (y_true - y_pred)**2. + log_var[0], -1)\n",
    "        return K.mean(loss)\n",
    "    \"\"\"\n",
    "    def multi_loss(self,  ys_true, ys_pred):\n",
    "        assert len(ys_true) == self.nb_outputs and len(ys_pred) == self.nb_outputs\n",
    "        loss = 0\n",
    "        i=0\n",
    "        for y_true, y_pred, log_var in zip(ys_true, ys_pred, self.log_vars):\n",
    "            precision =keras.backend.exp(-log_var[0]) \n",
    "            if i==0:\n",
    "                pred_loss = bbce(y_true, y_pred)\n",
    "                term = main_task_weight*precision*pred_loss + main_task_weight*0.5 * log_var[0]  \n",
    "                #term=tf.Print(keras.backend.mean(term), [keras.backend.mean(term)], 'mean bbce: ')\n",
    "            elif i==1:\n",
    "                # I need to find a better way for this\n",
    "                pred_loss = keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "                #keras_mse(y_true, y_pred)\n",
    "                term =  domain_weight * precision * pred_loss + domain_weight * log_var[0]\n",
    "                #term=tf.Print(keras.backend.mean(term), [keras.backend.mean(term)], 'mean cce: ')\n",
    "            else:\n",
    "                pred_loss = keras_mse(y_true, y_pred)\n",
    "                #pred_loss=tf.Print(pred_loss, [pred_loss], 'MSE: ')\n",
    "                term = 0.5 * precision * pred_loss + 0.5 * log_var[0]\n",
    "            loss+=term\n",
    "            term = 0.\n",
    "            i+=1\n",
    "        return keras.backend.mean(loss)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        ys_true = inputs[:self.nb_outputs]\n",
    "        ys_pred = inputs[self.nb_outputs:]\n",
    "        loss = self.multi_loss(ys_true, ys_pred)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return keras.backend.concatenate(inputs, -1)\n",
    "\n",
    "\"\"\"\n",
    "EVALUATION FUNCTIONs\n",
    "\"\"\"\n",
    "def accuracy_domain(y_true,y_pred):\n",
    "    y_p_r=np.round(y_pred)\n",
    "    acc = np.equal(y_p_r, y_true)**1.\n",
    "    acc = np.mean(np.float32(acc))\n",
    "    return acc\n",
    "def my_sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def my_accuracy_np(y_true, y_pred):\n",
    "    sliced_y_pred = my_sigmoid(y_pred)\n",
    "    y_pred_rounded = np.round(sliced_y_pred)\n",
    "    acc = np.equal(y_pred_rounded, y_true)**1.\n",
    "    acc = np.mean(np.float32(acc))\n",
    "    return acc\n",
    "def r_square_np(y_true, y_pred):\n",
    "    SS_res =  np.sum(np.square(y_true - y_pred))\n",
    "    SS_tot = np.sum(np.square(y_true - np.mean(y_true)))\n",
    "    r2_mine=( 1 - SS_res/(SS_tot + keras.backend.epsilon()) )\n",
    "    return ( 1 - SS_res/(SS_tot + keras.backend.epsilon()) )\n",
    "\n",
    "global report_val_acc \n",
    "global report_val_r2\n",
    "global report_val_mse\n",
    "report_val_acc=[]\n",
    "report_val_r2=[]\n",
    "report_val_mse=[]\n",
    "\n",
    "\"\"\"         \n",
    "Building guidable model \n",
    "\"\"\"\n",
    "def get_baseline_model(hp_lambda=0., c_list=[]):\n",
    "    base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "    layers_list=['conv2d_92', 'conv2d_93', 'conv2d_88', 'conv2d_89', 'conv2d_86']\n",
    "    #layers_list=[]\n",
    "    for i in range(len(base_model.layers[:])):\n",
    "        layer=base_model.layers[i]\n",
    "        if layer.name in layers_list:\n",
    "            print layer.name\n",
    "            layer.trainable=True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "    feature_output=base_model.layers[-1].output\n",
    "    gap_layer_output = keras.layers.GlobalAveragePooling2D()(feature_output)\n",
    "    feature_output = Dense(2048, activation='relu', name='finetuned_features1',kernel_regularizer=keras.regularizers.l2(0.01))(gap_layer_output) \n",
    "    feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "    feature_output = Dense(512, activation='relu', name='finetuned_features2',kernel_regularizer=keras.regularizers.l2(0.01))(feature_output)\n",
    "    feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "    feature_output = Dense(256, activation='relu', name='finetuned_features3',kernel_regularizer=keras.regularizers.l2(0.01))(feature_output)\n",
    "    feature_output = keras.layers.Dropout(0.8, noise_shape=None, seed=None)(feature_output)\n",
    "    grl_layer=GradientReversal(hp_lambda=hp_lambda)\n",
    "    feature_output_grl = grl_layer(feature_output)\n",
    "    domain_adversarial = keras.layers.Dense(7, activation = keras.layers.Activation('softmax'), name='domain_adversarial')(feature_output_grl)\n",
    "    finetuning = Dense(1,name='predictions')(feature_output)\n",
    "    ## here you need to check how many other concepts you have apart from domain adversarial\n",
    "    # then you add one layer per each. \n",
    "    output_nodes=[finetuning, domain_adversarial]\n",
    "    for c in c_list:\n",
    "        if c!='domain':\n",
    "            concept_layer=  keras.layers.Dense(1, activation = keras.layers.Activation('linear'), name='extra_{}'.format(c.strip(' ')))(feature_output)\n",
    "            output_nodes.append(concept_layer)\n",
    "    model = Model(input=base_model.input, output=output_nodes)\n",
    "    model.grl_layer=grl_layer\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_88\n",
      "conv2d_89\n",
      "conv2d_92\n",
      "conv2d_93\n",
      "conv2d_86\n",
      "1\n",
      "extra_0\n"
     ]
    }
   ],
   "source": [
    "main_task_weight=1. \n",
    "domain_weight = 1. #e-100\n",
    "model=get_baseline_model(hp_lambda=1., c_list=CONCEPT)\n",
    "t_m = get_trainable_model(model)\n",
    "#model=get_baseline_model(hp_lambda=1., c_list=CONCEPT)\n",
    "#t_m = get_trainable_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inp (InputLayer)                (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "y_true (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "domain_true (InputLayer)        (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "extra_0 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 [(None, 1), (None, 7 27181865    inp[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "custom_multi_loss_layer_1 (Cust [(None, 1), (None, 7 3           y_true[0][0]                     \n",
      "                                                                 domain_true[0][0]                \n",
      "                                                                 extra_0[0][0]                    \n",
      "                                                                 model_1[1][0]                    \n",
      "                                                                 model_1[1][1]                    \n",
      "                                                                 model_1[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 27,181,868\n",
      "Trainable params: 7,803,916\n",
      "Non-trainable params: 19,377,952\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "t_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def evaluate(pred_, save_file=None, c_list=CONCEPT):\n",
    "    y_true = pred_[:,0]\n",
    "    domain_true = pred_[:,1:8]\n",
    "    true_extra_concepts={}\n",
    "    if len(c_list)>1:\n",
    "        for i in range(1, len(c_list)):\n",
    "            true_extra_concepts[i]=pred_[:,8+i]\n",
    "            #print(i)\n",
    "            y_pred = pred_[:,8+i]\n",
    "            val_acc = my_accuracy_np(y_true, y_pred)\n",
    "            domain_pred = pred_[:, 8+i+1:8+i+1+7]\n",
    "            last_index=8+i+7\n",
    "    pred_extra_concepts={}\n",
    "    if len(c_list)>1:\n",
    "        for i in range(1, len(c_list)):\n",
    "            pred_extra_concepts[i]=pred_[:,last_index+i]\n",
    "    val_acc_d = accuracy_domain(domain_true, domain_pred)\n",
    "    val_r2={}\n",
    "    val_mse={}\n",
    "    if len(c_list)>1:\n",
    "        for i in range(1, len(c_list)):\n",
    "            val_r2[i] = r_square_np(true_extra_concepts[i], pred_extra_concepts[i])\n",
    "            val_mse[i] = compute_mse(true_extra_concepts[i], pred_extra_concepts[i])\n",
    "    \n",
    "    extra_string=''\n",
    "    if len(c_list)>1:\n",
    "        for i in range(1, len(c_list)):\n",
    "            extra_string=extra_string+\" {}: r2 {}, mse {}; \".format(i, val_r2[i], val_mse[i])\n",
    "    #print(\"Acc: {}, Acc domain: {}\\n\".format(val_acc, val_acc_d)+extra_string)\n",
    "    if save_file is not None:\n",
    "        save_file.write(\"Val acc: {}, acc_domain: {}\\n\".format(val_acc, val_acc_d)+extra_string)\n",
    "    return y_true, domain_true, true_extra_concepts, y_pred, domain_pred, pred_extra_concepts\n",
    "def compute_mse(labels, predictions):\n",
    "    errors = labels - predictions\n",
    "    sum_squared_errors = np.sum(np.asarray([pow(errors[i],2) for i in range(len(errors))]))\n",
    "    mse = sum_squared_errors / len(labels)\n",
    "    return mse\n",
    "def evaluate_model(d_list, model, batch_size=BATCH_SIZE, test_type=''):\n",
    "    batch_size=32\n",
    "    t_gen=DataGenerator(d_list, concept=CONCEPT, batch_size=BATCH_SIZE, data_type=0)\n",
    "    steps=len(d_list)//batch_size\n",
    "    initial_lr = 1e-4\n",
    "    opt = keras.optimizers.SGD(lr=initial_lr, momentum=0.9, nesterov=True)\n",
    "    compile_model(t_m,opt,loss=None,metrics=None)\n",
    "    callbacks = []\n",
    "    y_true=np.zeros(len(d_list))\n",
    "    y_pred=np.zeros((len(d_list),1))\n",
    "    N=0\n",
    "    all_true_domain=[]\n",
    "    all_pred_domain=[]\n",
    "    all_true_extra_cm={}#[]#np.zeros(len(d_list))\n",
    "    all_pred_extra_cm={}#[]#np.zeros(len(d_list))\n",
    "    batch_counter=0\n",
    "    while N<len(d_list):\n",
    "        #print N\n",
    "        input_,_ = t_gen.__getitem__(batch_counter)\n",
    "        pred_ = t_m.predict(input_)\n",
    "        \n",
    "        y_true_batch, d_true, true_ec, y_pred_batch, d_pred, pred_ec = evaluate(pred_)\n",
    "        #maybe some import pdb here\n",
    "        y_true[N:N+len(y_true_batch)]=y_true_batch.reshape(len(y_true_batch))\n",
    "        y_pred[N:N+len(y_pred_batch)]=y_pred_batch.reshape(len(y_pred_batch),1)\n",
    "        \n",
    "        all_true_domain.append(d_true)\n",
    "        all_pred_domain.append(d_pred)\n",
    "        for extra_concept in true_ec.keys():\n",
    "            try:\n",
    "                all_true_extra_cm[extra_concept].append(true_ec[extra_concept])\n",
    "            except:\n",
    "                all_true_extra_cm[extra_concept]=[]\n",
    "                all_true_extra_cm[extra_concept].append(true_ec[extra_concept])\n",
    "        for extra_concept in pred_ec.keys():\n",
    "            try:\n",
    "                all_pred_extra_cm[extra_concept].append(pred_ec[extra_concept])\n",
    "            except:\n",
    "                all_pred_extra_cm[extra_concept]=[]\n",
    "                all_pred_extra_cm[extra_concept].append(pred_ec[extra_concept])\n",
    "        N+=len(y_pred_batch)\n",
    "        batch_counter+=1\n",
    "    #import pdb; pdb.set_trace()\n",
    "    y_true=y_true.reshape((len(d_list),1))\n",
    "    #y_pred=y_pred.reshape((len(d_list),1))\n",
    "    acc = my_accuracy(y_true, y_pred).eval(session=tf.Session())\n",
    "    sliced_y_pred = tf.sigmoid(y_pred)\n",
    "    y_pred_rounded = K.round(sliced_y_pred)\n",
    "    acc_sc = accuracy_score(y_pred_rounded.eval(session=tf.Session()), y_true)\n",
    "    print('accuracy: ', acc_sc)\n",
    "    \n",
    "    y_pred = sliced_y_pred.eval(session=tf.Session())\n",
    "    #sliced_y_pred = tf.sigmoid(y_pred)\n",
    "    #y_pred_rounded = K.round(sliced_y_pred)\n",
    "    auc_score=sklearn.metrics.roc_auc_score(y_true,sliced_y_pred.eval(session=tf.Session()))\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(1):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    \n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc_score\n",
    "    plot=False\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "        plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "                 lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic example: {}'.format(roc_auc[0]))\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        auc_record = open('{}/auc_{}.txt'.format(model_folder,test_type), 'w')\n",
    "        auc_record.write('{}'.format(roc_auc[0]))\n",
    "        auc_record.close()\n",
    "    return all_true_domain, all_true_extra_cm, all_pred_domain, all_pred_extra_cm, roc_auc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder='/home/mara/multitask_adversarial/results/DOMAIN-COUNT/'\n",
    "CONCEPT=['domain','ncount']\n",
    "t_m.load_weights('{}/best_model.h5'.format(model_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list=test_list + test2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_true_domain_internal_test, all_true_extra_cm_internal_test, all_pred_domain_internal_test, all_pred_extra_cm_internal_test=evaluate_model(full_list,t_m, test_type='overall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_sample(data, n_samples=2):\n",
    "    sample_=[data[i] for i in np.random.choice(len(data),n_samples)]\n",
    "    #sample_=[data[i] for i in range(len(data))]\n",
    "    return sample_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7498653742595585)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7541733979536888)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7627894453419494)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7415185783521809)\n",
      "('accuracy: ', 0.7415185783521809)\n",
      "('accuracy: ', 0.7520193861066236)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7536348949919225)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7380183091007001)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7460958535271944)\n",
      "('accuracy: ', 0.7611739364566505)\n",
      "('accuracy: ', 0.749057619816909)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.747172859450727)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7552504038772213)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7520193861066236)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7539041464728056)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7482498653742595)\n",
      "('accuracy: ', 0.7568659127625202)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7506731287022078)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7522886375875067)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7576736672051696)\n",
      "('accuracy: ', 0.7439418416801292)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.750942380183091)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7485191168551427)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7477113624124933)\n",
      "('accuracy: ', 0.7460958535271944)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7474421109316102)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.749057619816909)\n",
      "('accuracy: ', 0.7625201938610663)\n",
      "('accuracy: ', 0.745288099084545)\n",
      "('accuracy: ', 0.7541733979536888)\n",
      "('accuracy: ', 0.7390953150242326)\n",
      "('accuracy: ', 0.7420570813139472)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7501346257404415)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7479806138933764)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7520193861066236)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.752827140549273)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.750942380183091)\n",
      "('accuracy: ', 0.7469036079698438)\n",
      "('accuracy: ', 0.7514808831448573)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7460958535271944)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7609046849757674)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7399030694668821)\n",
      "('accuracy: ', 0.7466343564889607)\n",
      "('accuracy: ', 0.7495961227786753)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7439418416801292)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7592891760904685)\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7630586968228326)\n",
      "('accuracy: ', 0.7474421109316102)\n",
      "AUC avg (std): 0.816104270799 (0.00615579038538)\n"
     ]
    }
   ],
   "source": [
    "keras.backend.get_session().run(tf.global_variables_initializer())\n",
    "t_m.load_weights(model_folder+'/best_model.h5')\n",
    "aucs_i=[]\n",
    "for i in range(50):\n",
    "    test_list_b=get_bootstrap_sample(full_list, n_samples=len(full_list))\n",
    "    all_cm_i, all_p_cm_i, _,_, roc_auc=evaluate_model(test_list_b,t_m, test_type='bootstrap_overall')\n",
    "    aucs_i.append(roc_auc)\n",
    "print \"AUC avg (std): {} ({})\".format(np.mean(aucs_i), np.std(aucs_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8097256593007709,\n",
       " 0.8257677583461371,\n",
       " 0.8284987543793056,\n",
       " 0.8123987690905612,\n",
       " 0.8122129193557766,\n",
       " 0.8171528762435989,\n",
       " 0.8203942669480053,\n",
       " 0.801909346122524,\n",
       " 0.8110533838973162,\n",
       " 0.8192357512953368,\n",
       " 0.8163210561918313,\n",
       " 0.8049622015028518,\n",
       " 0.8164248807485958,\n",
       " 0.8261588579545603,\n",
       " 0.8224349505654979,\n",
       " 0.8198083365432877,\n",
       " 0.8233338473508035,\n",
       " 0.8151898394372206,\n",
       " 0.8116935267976655,\n",
       " 0.8230061920156451,\n",
       " 0.80999705999706,\n",
       " 0.8225011543919973,\n",
       " 0.8194816813770406,\n",
       " 0.8124183815864469,\n",
       " 0.8216119126421779,\n",
       " 0.8131742798952575,\n",
       " 0.8133916206624894,\n",
       " 0.8185327317394306,\n",
       " 0.8132800425609821,\n",
       " 0.826003441176462,\n",
       " 0.81668630997849,\n",
       " 0.8185735954190274,\n",
       " 0.8031802696797476,\n",
       " 0.8128920345130592,\n",
       " 0.8152530177337852,\n",
       " 0.8212378998659381,\n",
       " 0.8154698586741791,\n",
       " 0.8117851422186724,\n",
       " 0.8090521202113915,\n",
       " 0.8107352601298298,\n",
       " 0.812407687849384,\n",
       " 0.8090276887279646,\n",
       " 0.8221827009936765,\n",
       " 0.8134452910776464,\n",
       " 0.8137599048373841,\n",
       " 0.8127480241741258,\n",
       " 0.812849532383189,\n",
       " 0.8179884057229148,\n",
       " 0.8306913580318795,\n",
       " 0.8171719576042143]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "[ERR]: tumor_extra17, tumor/level7/centre1/patient034/node3/patches, 483, ncount with path tumor_extra17/tumor/level7/centre1/patient034/node3/patches/483/ncount\n",
      "('accuracy: ', 0.7959073774905762)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-26d335851d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest_list_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_bootstrap_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mall_cm_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_p_cm_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_list_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bootstrap_internal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0maucs_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"AUC avg (std): {} ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maucs_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maucs_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "aucs=[]\n",
    "for i in range(50):\n",
    "    test_list_b=get_bootstrap_sample(test_list, n_samples=len(full_list))\n",
    "    all_cm_i, all_p_cm_i, roc_auc=evaluate_model(test_list_b,t_m, test_type='bootstrap_internal')\n",
    "    aucs_i.append(roc_auc)\n",
    "print \"AUC avg (std): {} ({})\".format(np.mean(aucs_i), np.std(aucs_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_i=[]\n",
    "for i in range(50):\n",
    "    test_list_b=get_bootstrap_sample(test2_list, n_samples=len(full_list))\n",
    "    all_cm_i, all_p_cm_i, roc_auc=evaluate_model(test_list_b,t_m, test_type='bootstrap_external')\n",
    "    aucs_i.append(roc_auc)\n",
    "print \"AUC avg (std): {} ({})\".format(np.mean(aucs_i), np.std(aucs_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
